{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZP2f2lCzDBU"
   },
   "source": [
    "# Machine Learning for Software Analysis (MLSA)\n",
    "\n",
    "### University of Florence / IMT Lucca\n",
    "\n",
    "#### Fabio Pinelli\n",
    "<a href=\"mailto:fabio.pinelli@imtlucca.it\">fabio.pinelli@imtlucca.it</a><br/>\n",
    "IMT School for Advanced Studies Lucca<br/>\n",
    "2023/2024<br/>\n",
    "October, 3 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLNxHhMphaId"
   },
   "source": [
    "## The promised assignments\n",
    "\n",
    "In the GitHub you can find 2 files:\n",
    "\n",
    "```\n",
    "data/assignments/us_companies.csv\n",
    "data/assignments/us_state_taxes.csv\n",
    "```\n",
    "\n",
    "The first one contains some information regarding a certain number of US companies\n",
    "\n",
    "The second one contains the percentage of taxes that a company should pay in each US state.\n",
    "\n",
    "\n",
    "Consider these files:\n",
    "- The states sorted by number of companies\n",
    "- Compute the average earnings of all the companies\n",
    "- Compute the average earnings of all the companies in California\n",
    "- Compute the average earnings of all the companies in California AND Texas\n",
    "- Create a new dataframe (and use ```.to_csv()``` to save in a csv) that contains a column where you computed the amount of taxes each company should pay\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE4J_HuuzLO0"
   },
   "source": [
    "# Lecture 2\\*: Introduction to *unsupervised learning*\n",
    "* Introduction\n",
    "* K-means\n",
    "* Other clustering methods\n",
    "* One application on a real dataset and relative visualizations\n",
    "* Dimensionality reduction (PCA) for clustering\n",
    "\n",
    "\\* with Lecture I mean \"argument\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juuwZIJsCZCO"
   },
   "source": [
    "## Taxonomy of Machine Learning\n",
    "\n",
    "-  Roughly speaking there are two main **categories** of ML tasks:\n",
    "\n",
    "    1.  **Supervised Learning**: Given a set of **labeled** examples, **predict** the labels of _new and unseen_ examples\n",
    "      + Classification\n",
    "      + Regression\n",
    "    2.  **Unsupervised Learning**: Given a set of examples, **find structure** in the data (e.g., _clusters_, _subspaces_, _manifolds_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSkesUn59znN"
   },
   "source": [
    "## What is and Why **clustering**?\n",
    "\n",
    "- It is a set of methods used to find group of objects in such a way that objects in the same group (called a *cluster*) are more similar (**in some sense**) to each other than to those in other groups (clusters).\n",
    "\n",
    "- Given a definition of distance (Euclidean?) (or *similarity*)\n",
    "\n",
    "- Not all the data can have labels, but still you need to find *similar* objects in your dataset\n",
    "\n",
    "- Can have a variety of applications:\n",
    "  - Customer segmentation\n",
    "  - Dimensionality reduction techniques as part of the ML process\n",
    "  - Anomaly detection\n",
    "  - Semi-supervised learning\n",
    "  - etc\n",
    "\n",
    "- We will explore a couple of methods:\n",
    "  - K-means (and some of its variants)\n",
    "  - DB-scan (so called density-based clustering)\n",
    "  - Quickly some other methods (e.g. hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s2HoKYEEeyE"
   },
   "source": [
    "\n",
    "## A quick introduction to **<code>scikit-learn</code>**:\n",
    "- It is a powerful library to handle the entire ML pipeline\n",
    " - Data pre-processing\n",
    " - Modeling (training-test split/cross validation/grid search)\n",
    " - Error evaluation\n",
    "\n",
    "- It is based on simple concepts:\n",
    " - fit\n",
    " - fit_transform\n",
    " - inverse_transform\n",
    " - predict\n",
    "\n",
    " - Scikit-learn is well integrated with many other Python libraries, such as <code>Matplotlib</code> and <code>plotly</code> for plotting, <code>NumPy</code> for array vectorization, <code>Pandas</code> dataframes, <code>SciPy</code>, and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0l7Xlrmwy7YP"
   },
   "source": [
    "\n",
    "## Setup\n",
    "\n",
    "A series of common commands to settle our working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cDpcWVxy7YQ"
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"unsupervised_learning\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58d-MNLhy7YQ"
   },
   "source": [
    "# Clustering\n",
    "\n",
    "- It is a set of methods used to find group of objects in such a way that objects in the same group (called a *cluster*) are more similar (**in some sense**) to each other than to those in other groups (clusters).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42c8CLMX4PIj"
   },
   "source": [
    "## K-means\n",
    "it aims to partition **$n$** observations into **$k$** clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or **cluster centroid**), serving as a prototype of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVg7R_D2y7YU"
   },
   "source": [
    "Let's start by generating some blobs (it is a way to generate synthetic data with a sort of predefined shape).\n",
    "\n",
    "We will explore the K-means procedure using a generated datasets using a scikit function *make_blobs*.\n",
    "\n",
    "\n",
    "``` python\n",
    "# Generate isotropic Gaussian blobs for clustering.\n",
    "\n",
    "sklearn.datasets.make_blobs(n_samples=100, n_features=2, *, centers=None,\n",
    "cluster_std=1.0, center_box=- 10.0, 10.0, shuffle=True, random_state=None,\n",
    "return_centers=False)\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nI4v-C3Cy7YU"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWPgc-L-y7YU"
   },
   "outputs": [],
   "source": [
    "#centers of our blobs of points\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "# standard deviation of our blobs to indicate the dispersion of each cluster\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0RKsh74y7YV"
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKfZjvix0HPS",
    "outputId": "f509cfbf-5e09-4b02-e316-fcaea4440c20"
   },
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xi-PF_H-uL1Z"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let's create the pandas dataframes so it might be easier for us to explore the data\n",
    "'''\n",
    "X = pd.DataFrame(X,columns=['feature1','feature2'])\n",
    "y = pd.DataFrame(y,columns=['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KJ16KNZ-vRTD",
    "outputId": "ea83065e-d60e-440d-a01a-e294927ab3b5"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(\"How many clusters should we expect? {}\".format(y.nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "DJcK2R9lvlDe",
    "outputId": "16f7547b-c3ec-49ce-b277-42e68d1b3842"
   },
   "outputs": [],
   "source": [
    "'''\"Which is the frequency of each cluster?'''\n",
    "y.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GMztFbby7YV"
   },
   "source": [
    "Now let's plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twA8cBmxy7YV"
   },
   "outputs": [],
   "source": [
    "def plot_clusters(X, y=None,centers=None):\n",
    "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, s=1)\n",
    "    if centers is not None:\n",
    "      plt.scatter(centers[:,0],centers[:,1],marker='x',c='red',s=40)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "tA08nj-Jy7YV",
    "outputId": "bfb3eccf-552c-4d0f-f4bf-bafb94c69525"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plot centers and the relative blobs\n",
    "'''\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_clusters(X,'blue',blob_centers)\n",
    "# save_fig(\"blobs_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDQzLmPey7YV"
   },
   "source": [
    "### Fit and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-BDVgvUy7YV"
   },
   "source": [
    "Let's train a K-Means clusterer on this dataset. It will try to find each blob's center and assign each instance to the closest blob:\n",
    "\n",
    "``` python\n",
    "class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init=10\n",
    "max_iter=300, tol=0.0001, precompute_distances='deprecated', verbose=0,\n",
    "random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fsKjyLwy7YW"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvO8z3Cuy7YW"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Find 5 clusters k = 5, clearly in this case, setting k is quite easy, since we already know the number of clusters.\n",
    "Later, we will explore how to find the right number of clusters when it is 'not known'\n",
    "'''\n",
    "\n",
    "k = 5\n",
    "# first we create the object kmeans with its parameters, then we fit (and predict)\n",
    "# on the data\n",
    "kmeans = KMeans(n_clusters=k, random_state=42,n_init=10)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vYJoVB8oy7YW",
    "outputId": "02720b48-f79e-492c-aea7-575f7d9c662d"
   },
   "outputs": [],
   "source": [
    "# Each instance was assigned to one of the 5 clusters:\n",
    "\n",
    "print('The shape of the predicted values is {}'.format(y_pred.shape))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kxv_M0oRy7YW",
    "outputId": "1b28f38a-f740-486f-8e56-41ca27c41a98"
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "All the labels are store in the attribute .labels_ of the kmeans object\n",
    "'''\n",
    "y_pred is kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z09GeQZh2enT",
    "outputId": "b0c0716b-b58b-4109-abce-21c2f91c46d7"
   },
   "outputs": [],
   "source": [
    "\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSSunxCKy7YW",
    "outputId": "3ff96a4f-7d27-4f20-86f0-eee5d1263e82"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "And the following 5 _centroids_ (i.e., cluster centers) were estimated:\n",
    "'''\n",
    "for i,c in enumerate(kmeans.cluster_centers_):\n",
    "  print('Cluster {} with centroid ({},({})'.format(i,c[0],c[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3M8ORfenKlst"
   },
   "source": [
    "Defining functions let us to be more clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbI7puk1xrlg"
   },
   "outputs": [],
   "source": [
    "def compare_centroids(centroids_1,centroids_2):\n",
    "  '''\n",
    "  it plots the two arrays of centroids, the first one is for the real ones,\n",
    "  the second for the ones estimated through clustering\n",
    "  '''\n",
    "  plt.scatter(centroids_1[:,0],centroids_1[:,1],marker='o',c='red',label='True')\n",
    "  plt.scatter(centroids_2[:,0],centroids_2[:,1],marker='x',c='blue',label='Estimated')\n",
    "  plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "rcVIgfs9yGHN",
    "outputId": "44fe8864-6723-47ec-c26a-7f28b41fcddc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Check if the discovered centroids are close to the real ones. Not bad at all :-)\n",
    "'''\n",
    "compare_centroids(blob_centers,kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhvnguyry7YX"
   },
   "source": [
    "Note that the `KMeans` instance preserves the labels of the instances it was trained on. Somewhat confusingly, in this context, the _label_ of an instance is the index of the cluster that instance gets assigned to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgLWQdIOy7YX",
    "outputId": "61d3d5fe-a652-4d77-8d79-e05aa666569e"
   },
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "bX2EMm01038B",
    "outputId": "20eeb473-8e58-42b7-ec2e-64aff87ad55f"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "check if we clustered all the points in a correct way.\n",
    "We create a new column in our dataset containing the cluster id, and we count the frequency\n",
    "for each assigned label\n",
    "'''\n",
    "X['cluster_id'] = kmeans.labels_\n",
    "X.cluster_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA1-jhk8y7YX"
   },
   "source": [
    "### Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7tnqmAFy7YX"
   },
   "source": [
    "Let's plot the model's decision boundaries. This gives us a _Voronoi diagram_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtocUCA4y7YX"
   },
   "outputs": [],
   "source": [
    "def plot_data(X):\n",
    "    plt.plot(X.iloc[:, 0], X.iloc[:, 1], 'k.', markersize=2)\n",
    "\n",
    "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
    "    if weights is not None:\n",
    "        centroids = centroids[weights > weights.max() / 10]\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='o', s=35, linewidths=8,\n",
    "                color=circle_color, zorder=10, alpha=0.9)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='x', s=2, linewidths=12,\n",
    "                color=cross_color, zorder=11, alpha=1)\n",
    "\n",
    "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
    "                             show_xlabels=True, show_ylabels=True):\n",
    "    mins = X.min(axis=0) - 0.1\n",
    "    maxs = X.max(axis=0) + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
    "                         np.linspace(mins[1], maxs[1], resolution))\n",
    "\n",
    "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')\n",
    "    plot_data(X)\n",
    "    if show_centroids:\n",
    "        plot_centroids(clusterer.cluster_centers_)\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "myqzndEAy7YY",
    "outputId": "e6d0335c-a1be-47cb-e982-bcd4b3f9b378"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans, X)\n",
    "# save_fig(\"voronoi_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBVL1pcMy7YY"
   },
   "source": [
    "Not bad! Some of the instances near the edges were probably assigned to the wrong cluster, but overall it looks pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSiasY9n1hNf"
   },
   "source": [
    "We can also *predict* new instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iE0SgC3w1YRw",
    "outputId": "5c3ec1fc-051c-4674-faea-0a8d8c4b1f5a"
   },
   "outputs": [],
   "source": [
    "X_new = pd.DataFrame(data=np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]]),columns=['feature1','feature2'])\n",
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcXlqdw6y7YY"
   },
   "source": [
    "### Hard Clustering _vs_ Soft Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYqI01qgy7YZ"
   },
   "source": [
    "Rather than arbitrarily choosing the closest cluster for each instance, which is called _hard clustering_, it might be better measure the distance of each instance to all 5 centroids. This is what the `transform()` method does:\n",
    "\n",
    "``` python\n",
    "transform(X)\n",
    "\n",
    "Transform X to a cluster-distance space.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7m9hqWOGy7YZ",
    "outputId": "d0d9a09f-56e5-45ff-f15a-9646a576e4cd"
   },
   "outputs": [],
   "source": [
    "kmeans.transform(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9alGPVmoy7YZ"
   },
   "source": [
    "You can verify that this is indeed the Euclidian distance between each instance and each centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7fOQDO8y7YZ",
    "outputId": "746e55b5-08f6-4275-8875-2d226b8dc06d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compute the norm --> distance using\n",
    "the numpy linear algebra\n",
    "From the documentation: The NumPy linear algebra functions rely on BLAS and LAPACK\n",
    "to provide efficient low level implementations of standard linear algebra algorithms.\n",
    "'''\n",
    "\n",
    "np.linalg.norm(np.tile(X_new, (1, k)).reshape(-1, k, 2) - kmeans.cluster_centers_, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbFVP8any7YZ"
   },
   "source": [
    "### K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6OpjThgy7YZ"
   },
   "source": [
    "The K-Means algorithm is one of the fastest clustering algorithms, and also one of the simplest:\n",
    "* First initialize $k$ centroids randomly: $k$ distinct instances are chosen randomly from the dataset and the centroids are placed at their locations.\n",
    "* Repeat until convergence (i.e., until the centroids stop moving):\n",
    "    * Assign each instance to the closest centroid.\n",
    "    * Update the centroids to be the mean of the instances that are assigned to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDZkVY5xy7YZ"
   },
   "source": [
    "The `KMeans` class applies an optimized algorithm by default. To get\n",
    "\n",
    "---\n",
    "\n",
    "the original K-Means algorithm (for educational purposes only), you must set `init=\"random\"`, `n_init=1`and `algorithm=\"lloyd\"`. These hyperparameters will be explained below.\n",
    "\n",
    "\n",
    "`\"auto\"` and `\"full\"` are deprecated and they will be removed in Scikit-Learn 1.3. They are both aliases for `\"lloyd\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A79-5xvy7Ya"
   },
   "source": [
    "Let's run the K-Means algorithm for 1, 2 and 3 iterations, to see how the centroids move around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "wuwbkzjGy7Ya",
    "outputId": "43d46da9-c254-45ff-cbed-a2049687df43"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "max_iter=1\n",
    "max_iter=2\n",
    "max_iter=3\n",
    "'''\n",
    "\n",
    "kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
    "                     algorithm=\"lloyd\", max_iter=1, random_state=0)\n",
    "kmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
    "                     algorithm=\"lloyd\", max_iter=2, random_state=0)\n",
    "kmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
    "                     algorithm=\"lloyd\", max_iter=3, random_state=0)\n",
    "kmeans_iter1.fit(X.iloc[:,[0,1]])\n",
    "kmeans_iter2.fit(X.iloc[:,[0,1]])\n",
    "kmeans_iter3.fit(X.iloc[:,[0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxFHXzTEy7Ya"
   },
   "source": [
    "And let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "IRLgOiIrxODo",
    "outputId": "984b19e0-b4ce-41cc-fb52-0bb352190dbe"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 922
    },
    "id": "cBbTuNhHy7Ya",
    "outputId": "4a6cb3aa-9e41-4707-a46b-1d13bd7302b8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.subplot(321)\n",
    "plot_data(X.iloc[:,[0,1]])\n",
    "plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n",
    "plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "plt.tick_params(labelbottom=False)\n",
    "plt.title(\"Update the centroids (initially randomly)\", fontsize=14)\n",
    "\n",
    "plt.subplot(322)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)\n",
    "plt.title(\"Label the instances\", fontsize=14)\n",
    "\n",
    "plt.subplot(323)\n",
    "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)\n",
    "plot_centroids(kmeans_iter2.cluster_centers_)\n",
    "\n",
    "plt.subplot(324)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)\n",
    "\n",
    "plt.subplot(325)\n",
    "plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n",
    "plot_centroids(kmeans_iter3.cluster_centers_)\n",
    "\n",
    "plt.subplot(326)\n",
    "plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n",
    "\n",
    "# save_fig(\"kmeans_algorithm_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k63_pDfAy7Ya"
   },
   "source": [
    "### K-Means Variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCba0VcNy7Ya"
   },
   "source": [
    "In the original K-Means algorithm, the centroids are just initialized randomly, and the algorithm simply runs a single iteration to gradually improve the centroids, as we saw above.\n",
    "\n",
    "However, one major problem with this approach is that if you run K-Means multiple times (or with different random seeds), it can converge to very different solutions, as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnNo7-Aky7Ya"
   },
   "outputs": [],
   "source": [
    "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n",
    "    clusterer1.fit(X)\n",
    "    clusterer2.fit(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plot_decision_boundaries(clusterer1, X)\n",
    "    if title1:\n",
    "        plt.title(title1, fontsize=14)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
    "    if title2:\n",
    "        plt.title(title2, fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "MVIkyudly7Yb",
    "outputId": "411f09b7-39f2-490b-8027-2d9b3d7b0f2a"
   },
   "outputs": [],
   "source": [
    "kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
    "                         algorithm=\"lloyd\", random_state=2)\n",
    "kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1,\n",
    "                         algorithm=\"lloyd\", random_state=5)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X.iloc[:,[0,1]],\n",
    "                          \"Solution 1\", \"Solution 2 (with a different random init)\")\n",
    "\n",
    "# save_fig(\"kmeans_variability_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9-zQcPty7Yb"
   },
   "source": [
    "### Inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Cudq_eWy7Yb"
   },
   "source": [
    "To select the best model, we will need a way to evaluate a K-Mean model's performance. Unfortunately, clustering is an unsupervised task, so we do not have the targets, and therefore a precise concept of **error**.\n",
    "\n",
    "But at least we can measure the distance between each instance and its centroid. This is the idea behind the _inertia_ metric:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1Clb_j7MXW2"
   },
   "source": [
    "\n",
    "It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster.\n",
    "\n",
    "\n",
    "A good model is:\n",
    "- one with low inertia AND\n",
    "- a low number of clusters ( $k$ )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6oLzAEfy7Yb",
    "outputId": "14eb8961-f260-409c-aaeb-b3989b692ad4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "attribute of the kmeans class inertia_\n",
    "'''\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6LO_srG1y7Yb"
   },
   "source": [
    "As you can easily verify, inertia is the sum of the squared distances between each training instance and its closest centroid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfZ0jhCey7Yc",
    "outputId": "3754bf8f-dae7-41ed-9be7-2b5aefc47be6"
   },
   "outputs": [],
   "source": [
    "X_dist = kmeans.transform(X.iloc[:,[0,1]])\n",
    "np.sum(X_dist[np.arange(len(X_dist)), kmeans.labels_]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm4ewTPry7Yc"
   },
   "source": [
    "The `score()` method returns the negative inertia. Why negative? Well, it is because a predictor's `score()` method must always respect the \"_greater is better_\" rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOylleCDy7Yc",
    "outputId": "bd203e95-f59b-471c-ecc9-5585c86959ee"
   },
   "outputs": [],
   "source": [
    "kmeans.score(X.iloc[:,[0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIlYwZuoy7Yc"
   },
   "source": [
    "### Multiple Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrCJF2bxy7Yc"
   },
   "source": [
    "So one approach to solve the variability issue is to simply run the K-Means algorithm multiple times with different random initializations, and select the solution that minimizes the inertia. For example, here are the inertias of the two \"bad\" models shown in the previous figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLF7aozVy7Yc",
    "outputId": "1089308a-78c6-498c-ff4e-025f1e767f80"
   },
   "outputs": [],
   "source": [
    "kmeans_rnd_init1.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLr2Ex55y7Yc",
    "outputId": "d56337b0-779f-4a68-dceb-dec23f202408"
   },
   "outputs": [],
   "source": [
    "kmeans_rnd_init2.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qx_54quLy7Yd"
   },
   "source": [
    "As you can see, they have a higher inertia than the first \"good\" model we trained, which means they are probably worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_HaTCkMy7Yd"
   },
   "source": [
    "When you set the `n_init` hyperparameter, Scikit-Learn runs the original algorithm `n_init` times, and selects the solution that minimizes the inertia. By default, Scikit-Learn sets `n_init=10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "Xwl0m8fcy7Yd",
    "outputId": "13d4a6bc-7274-4e9c-fb01-57e260141e1f"
   },
   "outputs": [],
   "source": [
    "kmeans_rnd_10_inits = KMeans(n_clusters=5, init=\"random\", n_init=10,\n",
    "                              algorithm=\"lloyd\", random_state=2)\n",
    "kmeans_rnd_10_inits.fit(X.iloc[:,[0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzuwhRaoy7Yd"
   },
   "source": [
    "As you can see, we end up with the initial model, which is certainly the optimal K-Means solution (at least in terms of inertia, and assuming $k=5$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "yGm1oKFxy7Yd",
    "outputId": "2cb74d93-138d-4d8f-eb01-f7baeb1149a4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundaries(kmeans_rnd_10_inits, X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTOTEnDiy7Yd"
   },
   "source": [
    "### K-Means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwONFOhvy7Yd"
   },
   "source": [
    "Instead of initializing the centroids entirely randomly, it is preferable to initialize them using the following algorithm, proposed in a [2006 paper](https://goo.gl/eNUPw6) by David Arthur and Sergei Vassilvitskii:\n",
    "* Take one centroid $c_1$, chosen uniformly at random from the dataset.\n",
    "* Take a new center $c_i$, choosing an instance $\\mathbf{x}_i$ with probability: $D(\\mathbf{x}_i)^2$ / $\\sum\\limits_{j=1}^{m}{D(\\mathbf{x}_j)}^2$ where $D(\\mathbf{x}_i)$ is the distance between the instance $\\mathbf{x}_i$ and the closest centroid that was already chosen. This probability distribution ensures that instances that are further away from already chosen centroids are much more likely be selected as centroids.\n",
    "* Repeat the previous step until all $k$ centroids have been chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsliGR-Fy7Yd"
   },
   "source": [
    "The rest of the K-Means++ algorithm is just regular K-Means. With this initialization, the K-Means algorithm is much less likely to converge to a suboptimal solution, so it is possible to reduce `n_init` considerably. Most of the time, this largely compensates for the additional complexity of the initialization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pX8hQ_xy7Ye"
   },
   "source": [
    "To set the initialization to K-Means++, simply set `init=\"k-means++\"` (this is actually the default):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "32YUzoFQy7Ye",
    "outputId": "380ad257-a1ae-4eac-fa1e-12f4e8067526"
   },
   "outputs": [],
   "source": [
    "# just to check the default parameters\n",
    "\n",
    "KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-2pVsPRuy7Ye",
    "outputId": "d6b2785f-c7a3-45a9-9300-9b113b9dd004"
   },
   "outputs": [],
   "source": [
    "good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)\n",
    "kmeans.fit(X.iloc[:,[0,1]])\n",
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCQGwqbVy7Yi"
   },
   "source": [
    "### Finding the optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbb7Qllny7Yi"
   },
   "source": [
    "What if the number of clusters was set to a lower or greater value than 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "Gorzorijy7Yi",
    "outputId": "232ccf95-fafd-46fc-96d1-07aa0c1cad03"
   },
   "outputs": [],
   "source": [
    "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
    "\n",
    "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X.iloc[:,[0,1]], \"$k=3$\", \"$k=8$\")\n",
    "# save_fig(\"bad_n_clusters_plot\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcaMENaKy7Yj"
   },
   "source": [
    "Ouch, these two models don't look great. What about their inertias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cjaWk5rcy7Yj",
    "outputId": "ff69996a-1e39-49b5-dff1-fbc12481174c"
   },
   "outputs": [],
   "source": [
    "print(\"k=3 inertia: {}\".format(kmeans_k3.inertia_))\n",
    "print(\"k=8 inertia: {}\".format(kmeans_k8.inertia_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OV7ldfnRy7Yj"
   },
   "source": [
    "No, we cannot simply take the value of $k$ that minimizes the inertia, since it keeps getting lower as we increase $k$. Indeed, the more clusters there are, the closer each instance will be to its closest centroid, and therefore the lower the inertia will be. However, we can plot the inertia as a function of $k$ and analyze the resulting curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Py6tdDh3y7Yj"
   },
   "outputs": [],
   "source": [
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X.iloc[:,[0,1]])\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "aMj6A22ky7Yj",
    "outputId": "97fbfc63-919e-499c-ac88-e55378a0dbc5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow',\n",
    "             xy=(4, inertias[3]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "plt.axis([1, 8.5, 0, 1300])\n",
    "# save_fig(\"inertia_vs_k_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r62oqZBuy7Yj"
   },
   "source": [
    "As you can see, there is an elbow at $k=4$, which means that less clusters than that would be bad, and more clusters would not help much and might cut clusters in half. So $k=4$ is a pretty good choice. Of course in this example it is not perfect since it means that the two blobs in the lower left will be considered as just a single cluster, but it's a pretty good clustering nonetheless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "w9bxVNZwy7Yk",
    "outputId": "a1e657bb-317f-4db1-86c8-fb80c4d4c601"
   },
   "outputs": [],
   "source": [
    "plot_decision_boundaries(kmeans_per_k[4-1], X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmYKRObRy7Yk"
   },
   "source": [
    "Another approach is to look at the **_silhouette score_**\n",
    "1. the mean _silhouette coefficient_ over all the instances.\n",
    "2. An instance's silhouette coefficient is equal to $(b - a)/\\max(a, b)$ where\n",
    "  - $a$ is the mean distance to the other instances in the same cluster (it is the _mean intra-cluster distance_)\n",
    "  - $b$ is the _mean nearest-cluster distance_, that is the mean distance to the instances of the next closest cluster (defined as the one that minimizes $b$, excluding the instance's own cluster).\n",
    "3. The **silhouette coefficient** can vary between **-1** and **+1**:\n",
    "  - a coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters,\n",
    "  - a coefficient close to 0 means that it is close to a cluster boundary\n",
    "  - finally a coefficient close to -1 means that the instance may have been assigned to the wrong cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AagKfc7uy7Yk"
   },
   "source": [
    "Let's plot the silhouette score as a function of $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qz45qVvEy7Yk"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXfKM8ady7Yk",
    "outputId": "d2583629-8ef0-46e3-8ec3-360cbab20f07"
   },
   "outputs": [],
   "source": [
    "silhouette_score(X.iloc[:,[0,1]], kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJuMjrd3y7Yk"
   },
   "outputs": [],
   "source": [
    "silhouette_scores = [silhouette_score(X.iloc[:,[0,1]], model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "3QSfetS1y7Yk",
    "outputId": "5b7d6bb7-b64c-4f37-c7ec-8c3d0ed60bc4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
    "# save_fig(\"silhouette_score_vs_k_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HE4kVwcoy7Yn"
   },
   "source": [
    "As you can see, this visualization is much richer than the previous one: in particular, although it confirms that $k=4$ is a very good choice, but it also underlines the fact that $k=5$ is quite good as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPpHoZQPy7Yn"
   },
   "source": [
    "An even more informative visualization is given when you plot every instance's silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a _silhouette diagram_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "id": "y9XcZEM2y7Yn",
    "outputId": "e74212ff-c60f-4120-d60f-0b96c4e742c8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "for k in (3, 4, 5, 6):\n",
    "    plt.subplot(2, 2, k - 2)\n",
    "\n",
    "    y_pred = kmeans_per_k[k - 1].labels_\n",
    "    silhouette_coefficients = silhouette_samples(X.iloc[:,[0,1]], y_pred)\n",
    "\n",
    "    padding = len(X.iloc[:,[0,1]]) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "\n",
    "        color = mpl.cm.Spectral(i / k)\n",
    "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ticks.append(pos + len(coeffs) // 2)\n",
    "        pos += len(coeffs) + padding\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "    if k in (3, 5):\n",
    "        plt.ylabel(\"Cluster\")\n",
    "\n",
    "    if k in (5, 6):\n",
    "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.xlabel(\"Silhouette Coefficient\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "\n",
    "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(\"$k={}$\".format(k), fontsize=16)\n",
    "\n",
    "save_fig(\"silhouette_analysis_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQyEnKAcy7Yo"
   },
   "source": [
    "As you can see, $k=5$ looks like the best option here, as all clusters are roughly the same size, and they all cross the dashed line, which represents the mean silhouette score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DBw0D3Ay7Yo"
   },
   "source": [
    "### Limits of K-Means\n",
    " - set the number of $k$\n",
    " - run several times to avoid sub-optimal solutions\n",
    " - problem handling clusters with different density and not spherical shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVQ9NWoDy7Yo"
   },
   "outputs": [],
   "source": [
    "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
    "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
    "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
    "X2 = X2 + [6, -8]\n",
    "X = np.r_[X1, X2]\n",
    "y = np.r_[y1, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AoVS16L9HuA"
   },
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X,columns=['X1','X2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "HiJ59Ozey7Yo",
    "outputId": "39aaf881-224f-422d-edc3-9ed61e14fae6"
   },
   "outputs": [],
   "source": [
    "plot_clusters(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "1jxWRNmLy7Yo",
    "outputId": "bdd91eb6-74ca-4377-abeb-e5ff287eb95a"
   },
   "outputs": [],
   "source": [
    "kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=42)\n",
    "kmeans_bad = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_good.fit(X)\n",
    "kmeans_bad.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "OlglP42My7Yo",
    "outputId": "45d3a188-b9a1-443c-9288-604ec29a8097"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_decision_boundaries(kmeans_good, X)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_good.inertia_), fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
    "plt.title(\"Inertia = {:.1f}\".format(kmeans_bad.inertia_), fontsize=14)\n",
    "\n",
    "# save_fig(\"bad_kmeans_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h19NGMkNtBJ4"
   },
   "source": [
    "### K-means on iris dataset\n",
    "\n",
    "We repeat the tests performed on the synthetic dataset on a \"real\" one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "id": "-nUfeZhC97Ky",
    "outputId": "4f7dedf8-51aa-4af9-a1ea-82d1c084c4d0"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "'''\n",
    "The Iris Dataset contains four features (length and width of sepals and petals)\n",
    "of 50 samples of three species of Iris\n",
    "(Iris setosa, Iris virginica and Iris versicolor)\n",
    "'''\n",
    "data = load_iris()\n",
    "# X = data.data\n",
    "# y = data.target\n",
    "# data.target_names\n",
    "\n",
    "'''\n",
    "let's transform our data coming from sklearn in a pandas dataframe.\n",
    "Notice that the method will work as well with initial data object\n",
    "'''\n",
    "df_iris = pd.DataFrame(data= np.c_[data['data'], data['target']],\n",
    "                     columns= data['feature_names'] + ['target'])\n",
    "\n",
    "df_iris.head()\n",
    "\n",
    "'''\n",
    "we select the features and the target column.\n",
    "The target is used just for testing purposes\n",
    "'''\n",
    "features = [c for c in df_iris.columns if 'target' not in c]\n",
    "target = ['target']\n",
    "X = df_iris[features]\n",
    "y = df_iris[target]\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(9, 3.5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X[y['target']==0].iloc[:,2], X[y['target']==0].iloc[:,3], \"yo\", label=\"Iris setosa\")\n",
    "plt.plot(X[y['target']==1].iloc[:,2], X[y['target']==1].iloc[:,3], \"bs\", label=\"Iris versicolor\")\n",
    "plt.plot(X[y['target']==2].iloc[:,2], X[y['target']==2].iloc[:,3], \"g^\", label=\"Iris virginica\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(X.iloc[:, 2], X.iloc[:, 3], c=\"b\", marker=\".\")\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.tick_params(labelleft=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Om-Gy8hY_SKb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We test different values of k and we store the relative inertia values.\n",
    "'''\n",
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "s6s2RKbK_SAj",
    "outputId": "34335a76-f27e-476e-d128-478be414e59f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow',\n",
    "             xy=(3, inertias[2]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "# plt.axis([1, 8.5, 0, 200])\n",
    "# save_fig(\"inertia_vs_k_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZLWJQ3__R3j"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here we compute the silhouette_score given the previous models\n",
    "'''\n",
    "\n",
    "silhouette_scores = [silhouette_score(X, model.labels_)\n",
    "                     for model in kmeans_per_k[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "yg6a7bKf_RuP",
    "outputId": "00fa8163-4e95-4381-8295-7da8607abf22"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "# save_fig(\"silhouette_score_vs_k_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 923
    },
    "id": "VC2bwASO_Rk1",
    "outputId": "856037bd-c1cd-4319-ba96-a04da2c1e7be"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "same plot as before, we look how the silhouette_coefficients is distributed\n",
    "across the clusters.\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "for k in (2, 3, 4, 5, 6):\n",
    "    plt.subplot(2, 3, k - 1)\n",
    "\n",
    "    y_pred = kmeans_per_k[k - 1].labels_\n",
    "    silhouette_coefficients = silhouette_samples(X.iloc[:,[0,1]], y_pred)\n",
    "\n",
    "    padding = len(X.iloc[:,[0,1]]) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "        coeffs = silhouette_coefficients[y_pred == i]\n",
    "        coeffs.sort()\n",
    "\n",
    "        color = mpl.cm.Spectral(i / k)\n",
    "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        ticks.append(pos + len(coeffs) // 2)\n",
    "        pos += len(coeffs) + padding\n",
    "\n",
    "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
    "    if k in (3, 5):\n",
    "        plt.ylabel(\"Cluster\")\n",
    "\n",
    "    if k in (5, 6):\n",
    "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.xlabel(\"Silhouette Coefficient\")\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "\n",
    "    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n",
    "    plt.title(\"$k={}$\".format(k), fontsize=16)\n",
    "\n",
    "save_fig(\"silhouette_analysis_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXryiOyvC1-C"
   },
   "source": [
    "Let's plot how the clusters look like with two features (2D-plot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "H8QWRyh8_RVB",
    "outputId": "cd05e9a2-6f5b-4313-891c-5418bd9a4839"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "kmeans = kmeans_per_k[2]\n",
    "X['cluster_id'] = kmeans.labels_\n",
    "\n",
    "'''\n",
    "scatter plot in seaborn -- we can set a groupby column hue='' used to plot groups of data\n",
    "'''\n",
    "# we have 4 features, we plot two of them at the time\n",
    "sns.scatterplot(data=X,x=X['petal length (cm)'],y=X['petal width (cm)'], hue='cluster_id', palette='tab10')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "Ecpae5p6DuGd",
    "outputId": "2676167b-5940-4758-8cc9-02531b6659e6"
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.scatterplot(data=X,x=X['sepal length (cm)'],y=X['sepal width (cm)'],hue='cluster_id', palette='tab10')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyrDvPrty7Ye"
   },
   "source": [
    "### Accelerated K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuXVv1wKy7Ye"
   },
   "source": [
    "The K-Means algorithm can be significantly accelerated by avoiding many unnecessary distance calculations: this is achieved by exploiting the triangle inequality (given three points A, B and C, the distance AC is always such that AC ≤ AB + BC) and by keeping track of lower and upper bounds for distances between instances and centroids (see this [2003 paper](https://www.aaai.org/Papers/ICML/2003/ICML03-022.pdf) by Charles Elkan for more details)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIbGLN3wy7Ye"
   },
   "source": [
    "To use Elkan's variant of K-Means, just set `algorithm=\"elkan\"`. Note that it does not support sparse data, so by default, Scikit-Learn uses `\"elkan\"` for dense data, and `\"lloyd\"` (the regular K-Means algorithm) for sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-RoGFqEy7Ye",
    "outputId": "a5c8c792-de3a-4684-faba-56a619d2d23e"
   },
   "outputs": [],
   "source": [
    "%timeit -n 50 KMeans(algorithm=\"elkan\", random_state=42).fit(X.iloc[:,[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sUFAYuyPy7Yf",
    "outputId": "52497ec3-6d48-470a-ccec-2212904f0ff8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit -n 50 KMeans(algorithm=\"lloyd\", random_state=42).fit(X.iloc[:,[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vIieIs3y7Yf"
   },
   "source": [
    "There's no big difference in this case, as the dataset is fairly small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ly6oK2d6vGhE"
   },
   "source": [
    "### K-means at work\n",
    "\n",
    "#### Web crawled data of company web-sites\n",
    "\n",
    "We crawled the home-pages of around 370k firms in Italy.\n",
    "For each home-page we tried to collect a series of signals in order to build eventually a new index able to capture the quality of company web sites, and then study if there is a relation between the performance of the companies and this new index.\n",
    "\n",
    "The data has been collected in two phases: 1) we our own craler; 2) getting info through google lighthouse\n",
    "\n",
    "For each home-page we gathered many features, but during our experiments we proved that only few of them are really useful, and some combinations of them:\n",
    "\n",
    "- 'tested_url'\n",
    "- 'linkedin'\n",
    "- 'n_links'\n",
    "- 'unique_links'\n",
    "- 'links_in'\n",
    "- 'links_out'\n",
    "- 'unique_links_in'\n",
    "- 'unique_links_out'\n",
    "- 'y_m_wa'\n",
    "- 'n_images'\n",
    "- 'request_time'\n",
    "- 'security_header'\n",
    "- 'unknown'\n",
    "- 'text_len'\n",
    "- 'facebook'\n",
    "- 'instagram'\n",
    "- 'security_header_int'\n",
    "- 'years_old'\n",
    "\n",
    "Other information describing the firm in terms of size, industry, location are contained in another csv populated using AIDA database.\n",
    "\n",
    "1. df with crawled features\n",
    "2. df with google features\n",
    "3. df with aida features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HmsNvLPvbiz",
    "outputId": "f86ce1e3-001a-4275-e0dd-223748236eab"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVN-YdNXwkXK",
    "outputId": "fc610084-c836-4556-e6b0-6ef58518ad98"
   },
   "outputs": [],
   "source": [
    "!pip install folium\n",
    "!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3lqyTVQwhv1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd # library to handle spatial data\n",
    "import folium # library to generate maps\n",
    "import seaborn as sns\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYL2tUqOY_El"
   },
   "source": [
    "#### Load the 3 above mentioned datasets:\n",
    " - crawled data\n",
    " - data gathered from google\n",
    " - orbis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxd9EH3-vbfh"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/Shareddrives/phd_hands_on/clustering_test/merged_web_full.csv',converters={'Codicefiscale':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "id": "aPDUgeR1ZNem",
    "outputId": "836e1b30-768c-4e1a-b718-31775bcf6757"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let's check some property of our first dataframe\n",
    "'''\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3qmHEe1Z061",
    "outputId": "4656c2a8-3e1d-4b60-d8b3-460198955703"
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVuajLq2Z4Kw",
    "outputId": "102e5699-39f5-44f7-ff93-51fc0690f47d"
   },
   "outputs": [],
   "source": [
    "\n",
    "print('The dataset contains {} rows and {} columns'.format(df.shape[0],df.shape[1]))\n",
    "print('The columns contained in our first dataframe are:\\n{}'.format( ',\\n'.join('{}. {}'.format(i,c) for i,c in enumerate(df.columns)) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqczBkXxalyB"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "After several experiments we select only some of the columns of our dataset.\n",
    "In this case we select the needed part of our dataframe using iloc\n",
    "'''\n",
    "\n",
    "df_sel = df.iloc[:,list(range(1,14))+[15,16,17,18,19,20,22,24,25]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EoKFaxSFa-sB",
    "outputId": "d605783a-b383-41a8-e2a5-2d353aab91c3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "let's check the new shape, what do you expect?\n",
    "There is another way to select only few columns?\n",
    "'''\n",
    "print('The new dataframe has this shape: {}'.format(df_sel.shape))\n",
    "print('The old dataset instead... {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbCOmOCubXkP"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We want to generate new columns building some \"stronger\" indicator of the web-site quality\n",
    "'''\n",
    "\n",
    "df_sel['ratio_links'] = df_sel['unique_links']/df_sel['n_links']\n",
    "\n",
    "df_sel['bad_images_ratio'] = df_sel['unknown']/df_sel['n_images']\n",
    "\n",
    "df_sel['ratio_links_in'] = df_sel['unique_links_in']/df_sel['links_in']\n",
    "\n",
    "df_sel['ratio_links_out'] = df_sel['unique_links_out']/df_sel['links_out']\n",
    "\n",
    "df_sel['length_url'] = df_sel['original_url'].apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQ5S91R_bX6_",
    "outputId": "47a31d75-27c7-4382-eaaa-578bdf39ab9d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now are dataset has the following shape\n",
    "'''\n",
    "print('The new dataframe has this shape: {}'.format(df_sel.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYI8gMyTbX-8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let's load the dataframe that contains the information gathered using google lighthouse\n",
    "'''\n",
    "\n",
    "df_google = pd.read_csv('/content/drive/Shareddrives/phd_hands_on/clustering_test/google_features.csv',converters={'Codicefiscale':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "UfQp9Tl8bYC0",
    "outputId": "ba9b27cd-0c4c-4d39-debb-2cd00a8ed521"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let's check some property of our second dataframe\n",
    "'''\n",
    "df_google.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xYnih2RcxRL",
    "outputId": "c2e5309b-d607-4312-ed5e-9ad729c39c84"
   },
   "outputs": [],
   "source": [
    "print('The dataset contains {} rows and {} columns'.format(df_google.shape[0],df.shape[1]))\n",
    "print('The columns contained in our first dataframe are:\\n{}'.format( ',\\n'.join('{}. {}'.format(i,c) for i,c in enumerate(df_google.columns)) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-1yj7uybYFW"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "It could be useful and easier to have a single dataframe to handle\n",
    "There is a common column between the two dataframes \"Codicefiscale\".\n",
    "So we can merge the two dataframes using the common column.\n",
    "In particular, we want all the firms that have both types of information --> inner join\n",
    "'''\n",
    "df_merged = pd.merge(df_sel,df_google,on='Codicefiscale',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Leo_rnOabYIE",
    "outputId": "322815b7-5902-4b9b-dd89-6873a10452fc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let's have a look on how the shape of the df_merged dataframe is\n",
    "'''\n",
    "print('The dataset contains {} rows and {} columns'.format(df_merged.shape[0],df_merged.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mu9eiHKbYKg"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now the last dataframe, the one obtained from aida\n",
    "For this one we select the columns by name.\n",
    "What could we use instead of [] notation?\n",
    "'''\n",
    "\n",
    "df_orbis = pd.read_csv('/content/drive/Shareddrives/phd_hands_on/clustering_test/panel_aziende_crawled.csv',converters={'Codicefiscale':str,'ATECO2007codice':str})\n",
    "\n",
    "orbis_columns = ['Codicefiscale','NUTS3','Classificazione per dimensione','NACE Rev. 2, sezione principale','Numero dipendenti\\n2019','Città\\nLocal Alphabet']\n",
    "\n",
    "df_orbis = df_orbis[orbis_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Awlt1xcBbYNB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f09_kONwbYPs"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "we want to get only the ID from the NUTS3\n",
    "'''\n",
    "df_orbis['NUTS_ID'] = df_orbis.NUTS3.apply(lambda x: x.split(' - ')[0] if type(x) != float else x )\n",
    "\n",
    "'''\n",
    "And fill the NaN with a special value NULL\n",
    "'''\n",
    "df_orbis.fillna(value={'NACE Rev. 2, sezione principale':'NULL'},inplace=True)\n",
    "\n",
    "\n",
    "'''\n",
    "from the industry/sector/ateco we take only the first digit (that is a letter)\n",
    "'''\n",
    "df_orbis['macro_sector'] = df_orbis['NACE Rev. 2, sezione principale'].apply(lambda x: x.split(' - ')[0])\n",
    "\n",
    "'''\n",
    "Then we merged the two dataframes\n",
    "'''\n",
    "df_full = pd.merge(df_merged,df_orbis,on='Codicefiscale',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gf_aAhEEFKIx",
    "outputId": "1d612f87-ce3d-44be-a66f-2c6ae3d287bd"
   },
   "outputs": [],
   "source": [
    "print(df_full.shape)\n",
    "print(df_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AevnIvgreepq"
   },
   "source": [
    "#### Aggregation\n",
    "\n",
    "Performing the clustering on all the firms can be tricky. For now, keep it simple.\n",
    "\n",
    "We aggregate the data by province (then by municipalities) and then we try to cluster the provinces and see if some spatial patterns emerge.\n",
    "\n",
    "How we perform the aggregation at province level?\n",
    "\n",
    "Pandas -- like <code>SQL</code> has the <code>groupby functionality</code>\n",
    "\n",
    "For each \"group\" in the dataframe it can computes several aggregation functions.\n",
    "\n",
    "In our case, the groups are the ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awFfmVEmbYSi"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The default behaviour of the groupby function is to use the groupby columns as index.\n",
    "In this case, we set the input parameter as_index=False, to force the groupby to\n",
    "use the groupby columns as other columns\n",
    "\n",
    "Second, in general it is possible to specify the aggregation functions in several ways.\n",
    "Using pd.NamedAgg let us decide the name of the columns containing the results of the aggregation.\n",
    "'''\n",
    "\n",
    "\n",
    "gb_df = df_full[df_full.NUTS_ID != 'ITF48'].groupby(['NUTS3','NUTS_ID'],as_index=False).agg(\n",
    "        total = pd.NamedAgg(column='Codicefiscale', aggfunc='count'),\n",
    "        instagrams = pd.NamedAgg(column='instagram', aggfunc='sum'),\n",
    "        linkedins = pd.NamedAgg(column='linkedin', aggfunc='sum'),\n",
    "        facebooks = pd.NamedAgg(column='facebook', aggfunc='sum'),\n",
    "\n",
    "        mean_images = pd.NamedAgg(column='n_images', aggfunc='mean'),\n",
    "        mean_bad_images = pd.NamedAgg(column='bad_images_ratio', aggfunc='mean'),\n",
    "        mean_links = pd.NamedAgg(column='n_links', aggfunc='mean'),\n",
    "        mean_links_in = pd.NamedAgg(column='links_in', aggfunc='mean'),\n",
    "        mean_links_out = pd.NamedAgg(column='links_out', aggfunc='mean'),\n",
    "        mean_ratio_links = pd.NamedAgg(column='ratio_links', aggfunc='mean'),\n",
    "        mean_ratio_links_in = pd.NamedAgg(column='ratio_links_in', aggfunc='mean'),\n",
    "        mean_ratio_links_out = pd.NamedAgg(column='ratio_links_out', aggfunc='mean'),\n",
    "        mean_years_old = pd.NamedAgg(column='years_old', aggfunc='mean'),\n",
    "        mean_security_header_int = pd.NamedAgg(column='security_header_int', aggfunc='mean'),\n",
    "        mean_text_len = pd.NamedAgg(column='text_len', aggfunc='mean'),\n",
    "        mean_request_time = pd.NamedAgg(column='request_time', aggfunc='mean'),\n",
    "        mean_length_url = pd.NamedAgg(column='length_url', aggfunc='mean'),\n",
    "        mean_accessibility = pd.NamedAgg(column='accessibility', aggfunc='mean'),\n",
    "        mean_best_practices = pd.NamedAgg(column='best-practices', aggfunc='mean'),\n",
    "        mean_performance = pd.NamedAgg(column='performance', aggfunc='mean'),\n",
    "        mean_seo = pd.NamedAgg(column='seo', aggfunc='mean'),\n",
    "\n",
    "        median_images = pd.NamedAgg(column='n_images', aggfunc='median'),\n",
    "        median_request_time = pd.NamedAgg(column='request_time', aggfunc='median'),\n",
    "        median_links = pd.NamedAgg(column='n_links', aggfunc='median'),\n",
    "        median_links_in = pd.NamedAgg(column='links_in', aggfunc='median'),\n",
    "        median_links_out = pd.NamedAgg(column='links_out', aggfunc='median'),\n",
    "        median_years_old = pd.NamedAgg(column='years_old', aggfunc='median'),\n",
    "        median_ratio_links = pd.NamedAgg(column='ratio_links', aggfunc='median'),\n",
    "        median_ratio_links_in = pd.NamedAgg(column='ratio_links_in', aggfunc='median'),\n",
    "        median_ratio_links_out = pd.NamedAgg(column='ratio_links_out', aggfunc='median'),\n",
    "        median_bad_images = pd.NamedAgg(column='bad_images_ratio', aggfunc='median'),\n",
    "        median_security_header_int = pd.NamedAgg(column='security_header_int', aggfunc='median'),\n",
    "        median_text_len = pd.NamedAgg(column='text_len', aggfunc='median'),\n",
    "        median_length_url = pd.NamedAgg(column='length_url', aggfunc='median'),\n",
    "        median_accessibility = pd.NamedAgg(column='accessibility', aggfunc='median'),\n",
    "        median_best_practices = pd.NamedAgg(column='best-practices', aggfunc='median'),\n",
    "        median_performance = pd.NamedAgg(column='performance', aggfunc='median'),\n",
    "        median_seo = pd.NamedAgg(column='seo', aggfunc='median'),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "gb_df['percentage_facebook'] = gb_df.facebooks/gb_df.total*100\n",
    "gb_df['percentage_instagram'] = gb_df.instagrams/gb_df.total*100\n",
    "gb_df['percentage_linkedin'] = gb_df.linkedins/gb_df.total*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "QFOEwvz9G_1v",
    "outputId": "e54dbd27-d43b-433b-dc31-3810a5cf2305"
   },
   "outputs": [],
   "source": [
    "\n",
    "gb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB9Ds6twvbWb"
   },
   "outputs": [],
   "source": [
    "def province_rename(x):\n",
    "    if x is not None:\n",
    "        province_map = {\n",
    "                    \"Valle D'Aosta\":'Aosta',\n",
    "                    \"Reggio Calabria\": 'Reggio di Calabria',\n",
    "                    \"Reggio Emilia\": \"Reggio nell'Emilia\",\n",
    "                    \"Forli-Cesena\":\"Forli'-Cesena\",\n",
    "                    \"Ogliastra\": \"Sud Sardegna\",\n",
    "                    \"Massa-Carrara\": \"Massa Carrara\",\n",
    "                    \"Pesaro E Urbino\": \"Pesaro e Urbino\"\n",
    "                       }\n",
    "\n",
    "        province = x.split('- ')[1]\n",
    "        if province in province_map.keys():\n",
    "            return province_map[province]\n",
    "        return province\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vj7h-j-4vbK7"
   },
   "outputs": [],
   "source": [
    "gb_df['DEN_UTS'] = gb_df['NUTS3'].apply(lambda x: province_rename(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYSNuQk1YzIB"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To visualize data on maps we need to know the spatial polygons that describe the provinces.\n",
    "This particular files are called shape files.\n",
    "We can handle this kind of file using geopandas library.\n",
    "It provides a series of spatial operations between dataframes containing\n",
    "a geometry and a spatial system reference\n",
    "'''\n",
    "province_shape = gpd.read_file('/content/drive/Shareddrives/phd_hands_on/clustering_test/ProvCM01012021_g_WGS84.shp',encoding='utf-8')[['DEN_UTS','SIGLA','geometry']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "W1G3fWUzHuu-",
    "outputId": "0ef86356-9c59-47cc-f58d-67b6203cb5bc"
   },
   "outputs": [],
   "source": [
    "\n",
    "province_shape.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AP_EnnIYzA6"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Merge once again to add the geometry\n",
    "'''\n",
    "\n",
    "df_province_geom = pd.merge(gb_df,province_shape,on='DEN_UTS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzASHLQAYy55",
    "outputId": "69009d56-62bf-45c8-a909-4b28c58981e5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Let's recap\n",
    "What we have in the dataset and how it looks like.\n",
    "'''\n",
    "\n",
    "df_province_geom.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JTSa0vDdYyzA",
    "outputId": "f5729be3-d125-45f4-a92a-57351d26722c"
   },
   "outputs": [],
   "source": [
    "df_province_geom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1qQulSFYyr1",
    "outputId": "7bdd8e4a-8a6c-472a-f674-7afa543ffab1"
   },
   "outputs": [],
   "source": [
    "df_province_geom.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7AdRMsShbKF"
   },
   "source": [
    "We don't have null values, do you have an idea why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYwY0Fq0Yykz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "And now some visualization.\n",
    "We have a dataframe that contains several features per province.\n",
    "The features, despite some ids, are all numerical columns,\n",
    "therefore we can plot them and visualize, for instance, in a fancy map :-)\n",
    "'''\n",
    "\n",
    "# The function takes in input a dataframe, the column to be visualized\n",
    "# and which column should be used for the geometry\n",
    "# It returns a Folium map object\n",
    "\n",
    "\n",
    "def map_df(df,column='percentage',column_gb = 'NUTS_ID'):\n",
    "    gdf_nuts_geom = gpd.GeoDataFrame(df,geometry='geometry')\n",
    "    gdf_nuts_geom.dropna(inplace=True)\n",
    "    m = folium.Map((41.9027835,12.4963655),zoom_start=6)\n",
    "    custom_scale = (df[column].quantile((0,0.33,0.66,1))).tolist()\n",
    "    folium.Choropleth(\n",
    "        geo_data=gdf_nuts_geom,\n",
    "        data=gdf_nuts_geom,\n",
    "        columns=[column_gb,column],\n",
    "        key_on=\"feature.properties.{}\".format(column_gb),\n",
    "        threshold_scale=custom_scale,\n",
    "        fill_color='YlGnBu',\n",
    "        fill_opacity=1,\n",
    "        line_opacity=0.5,\n",
    "        legend_name=\"wills\",\n",
    "        smooth_factor=0,\n",
    "        Highlight= True,\n",
    "        line_color = \"#0000\",\n",
    "        name = 'percentage',\n",
    "        show=False,\n",
    "        overlay=True,\n",
    "        nan_fill_color = \"White\"\n",
    "    ).add_to(m)\n",
    "    style_function = lambda x: {'fillColor': '#ffffff',\n",
    "                                'color':'#000000',\n",
    "                                'fillOpacity': 0.1,\n",
    "                                'weight': 0.1}\n",
    "\n",
    "    highlight_function = lambda x: {'fillColor': '#000000',\n",
    "                                    'color':'#000000',\n",
    "                                    'fillOpacity': 0.50,\n",
    "                                    'weight': 0.1}\n",
    "    NIL = folium.features.GeoJson(\n",
    "        data = gdf_nuts_geom,\n",
    "        style_function=style_function,\n",
    "        control=False,\n",
    "        highlight_function=highlight_function,\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=['NUTS3',column],\n",
    "            aliases=['NUTS3',column],\n",
    "            style=(\"background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;\")\n",
    "        )\n",
    "    )\n",
    "    m.add_child(NIL)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "V9BfIgxnYyei",
    "outputId": "d43a4d0e-5025-440b-f30e-510a702780ca"
   },
   "outputs": [],
   "source": [
    "map_df(df_province_geom,'percentage_instagram','DEN_UTS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bedDKKzLkE7m"
   },
   "source": [
    "#### Clustering, let's go for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzkFppD7YyQf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Some utility functions to color our beautiful maps\n",
    "'''\n",
    "\n",
    "def get_color(x):\n",
    "    colors=['#9e0142', '#e95c47', '#fdbf6f', '#ffffbe', '#bfe5a0', '#54aead', '#5e4fa2']\n",
    "    return colors[x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhFdVovCYyJb"
   },
   "outputs": [],
   "source": [
    "\n",
    "clustering_columns_mean_1 = [\n",
    " 'mean_best_practices',\n",
    " 'mean_seo',\n",
    " 'mean_ratio_links_in',\n",
    " 'mean_ratio_links_out',\n",
    " 'mean_years_old',\n",
    " 'mean_security_header_int',\n",
    " 'mean_request_time',\n",
    " 'mean_length_url',\n",
    " 'percentage_facebook',\n",
    " 'percentage_instagram',\n",
    " 'percentage_linkedin']\n",
    "\n",
    "# clustering_columns_mean_2 = ['mean_best_practices', 'mean_seo',]\n",
    "# clustering_columns_median_1 = [       'median_accessibility', 'median_best_practices', 'median_performance',\n",
    "#        'median_seo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diWmfMgDYyCJ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "For plotting reasons, when we use Latex engine we can't use the _\n",
    "so we need to handle it during the process -- nothing to worry about.\n",
    "Then, we select the columns selected for the clustering\n",
    "'''\n",
    "gb_df.columns = [c.replace('-','_')for c in gb_df.columns]\n",
    "df_clustering = gb_df[clustering_columns_mean_1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wULOmxqYx63",
    "outputId": "3b6715a8-3b01-483c-89e8-3b72d00e68e9"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In order to define a distance over several features it is necessary to rescale them on a similar space.\n",
    "Scikit provides a series of scaler, in this case we adopted the MinMax scaler.\n",
    "'''\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df_clustering)\n",
    "print(type(scaled))\n",
    "df_scaled = pd.DataFrame(scaled,columns=clustering_columns_mean_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "qrv3CuUpYxy2",
    "outputId": "d2e88a60-e761-411e-d923-e5e102493f2e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We perform the elbow test as we did it before.\n",
    "We test 10 k, and we plot the inertias\n",
    "'''\n",
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(df_scaled)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow',\n",
    "             xy=(3, inertias[2]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "# plt.axis([1, 8.5, 0, 200])\n",
    "#     plt.savefig('clustering/elbow_{}.png'.format(name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6RSi1M3YxoL"
   },
   "outputs": [],
   "source": [
    "n_clusters = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "id": "VxCmV0-GYxZi",
    "outputId": "37825540-49b8-447d-f5fe-f1aaee59f0f9"
   },
   "outputs": [],
   "source": [
    "km = kmeans_per_k[n_clusters-1]\n",
    "\n",
    "gb_df['cluster_id'] = km.labels_\n",
    "\n",
    "print(gb_df.cluster_id.value_counts())\n",
    "\n",
    "gb_df.columns = [c.replace('_','-') if c in clustering_columns_mean_1 else c for c in gb_df.columns]\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "for i,c in enumerate(km.cluster_centers_):\n",
    "    plt.plot(c,label='cluster {}'.format(i),marker='x',linewidth=3,color=get_color(i))\n",
    "\n",
    "ticks = list(range(0,len(clustering_columns_mean_1),1))\n",
    "columns = [c.replace('_','-') for c in clustering_columns_mean_1]\n",
    "plt_labels = clustering_columns_mean_1[::1]\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.ylabel('Mean value', fontsize=24)\n",
    "plt.xlabel('Clustering features', fontsize=24)\n",
    "\n",
    "plt.grid()\n",
    "plt.xticks(ticks=ticks,labels=plt_labels,rotation = 90)\n",
    "plt.legend(loc='best',fontsize=18)\n",
    "# plt.savefig('clustering/centroids_{}.png'.format(name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dj6mwEXfYwso",
    "outputId": "02345bc4-9a05-42da-d664-d7ebce81b4ee"
   },
   "outputs": [],
   "source": [
    "df_nuts_geom = pd.merge(gb_df,province_shape,on='DEN_UTS')\n",
    "\n",
    "df_nuts_geom['rgba'] = df_nuts_geom.cluster_id.apply(lambda x: get_color(x).lower())\n",
    "\n",
    "gdf_nuts_geom = gpd.GeoDataFrame(df_nuts_geom,geometry='geometry')\n",
    "gdf_nuts_geom.dropna(inplace=True)\n",
    "\n",
    "m = folium.Map((41.9027835,12.4963655),zoom_start=6)\n",
    "folium.GeoJson(\n",
    "    gdf_nuts_geom,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': feature['properties']['rgba'],\n",
    "        'color' : feature['properties']['rgba'],\n",
    "        'weight' : 1,\n",
    "        'fillOpacity' : 0.7,\n",
    "        },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['NUTS3','cluster_id'],\n",
    "        aliases=['Provincia','Cluster'],\n",
    "        localize=True\n",
    "    )\n",
    "\n",
    "    ).add_to(m)\n",
    "\n",
    "# folium.map.CustomPane('labels').add_to(m)\n",
    "#     folium.TileLayer('CartoDBPositronOnlyLabels',\n",
    "#                      pane='labels').add_to(m)\n",
    "#m.save('clustering/clustering_{}.html'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "fvroeiXBla8N",
    "outputId": "9e83bb8f-c3ef-45a3-93fc-80a205b47e02"
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRQeks1XltId"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Well, if we want to test different sets of features, what we could to to authomatize the process??\n",
    "'''\n",
    "'''\n",
    "We can define a function where we pass the columns to be used in the clustering\n",
    "'''\n",
    "\n",
    "def clustering(gb_df,columns,province_shape,name,n_clusters=3):\n",
    "    gb_df.columns = [c.replace('-','_')for c in gb_df.columns]\n",
    "    df_clustering = gb_df[columns].copy()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled = scaler.fit_transform(df_clustering)\n",
    "\n",
    "    df_scaled = pd.DataFrame(scaled,columns=columns)\n",
    "\n",
    "    kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(df_scaled)\n",
    "                    for k in range(1, 10)]\n",
    "    inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "    plt.figure(figsize=(8, 3.5))\n",
    "    plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "    plt.xlabel(\"$k$\", fontsize=14)\n",
    "    plt.ylabel(\"Inertia\", fontsize=14)\n",
    "    plt.annotate('Elbow',\n",
    "                 xy=(3, inertias[2]),\n",
    "                 xytext=(0.55, 0.55),\n",
    "                 textcoords='figure fraction',\n",
    "                 fontsize=16,\n",
    "                 arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "                )\n",
    "    # plt.axis([1, 8.5, 0, 200])\n",
    "#     plt.savefig('clustering/elbow_{}.png'.format(name))\n",
    "    plt.show()\n",
    "\n",
    "    km = kmeans_per_k[n_clusters-1]\n",
    "\n",
    "    gb_df['cluster_id'] = km.labels_\n",
    "\n",
    "    print(gb_df.cluster_id.value_counts())\n",
    "\n",
    "    gb_df.columns = [c.replace('_','-') if c in columns else c for c in gb_df.columns]\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    for i,c in enumerate(km.cluster_centers_):\n",
    "        plt.plot(c,label='cluster {}'.format(i),marker='x',linewidth=3,color=get_color(i))\n",
    "\n",
    "    ticks = list(range(0,len(columns),1))\n",
    "    columns = [c.replace('_','-') for c in columns]\n",
    "    plt_labels = columns[::1]\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    plt.ylabel('Mean value', fontsize=24)\n",
    "    plt.xlabel('Clustering features', fontsize=24)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xticks(ticks=ticks,labels=plt_labels,rotation = 90)\n",
    "    plt.legend(loc='best',fontsize=18)\n",
    "    # plt.savefig('clustering/centroids_{}.png'.format(name))\n",
    "    plt.show()\n",
    "\n",
    "    df_nuts_geom = pd.merge(gb_df,province_shape,on='DEN_UTS')\n",
    "\n",
    "    df_nuts_geom['rgba'] = df_nuts_geom.cluster_id.apply(lambda x: get_color(x).lower())\n",
    "\n",
    "    gdf_nuts_geom = gpd.GeoDataFrame(df_nuts_geom,geometry='geometry')\n",
    "    gdf_nuts_geom.dropna(inplace=True)\n",
    "\n",
    "    m = folium.Map((41.9027835,12.4963655),zoom_start=6)\n",
    "    folium.GeoJson(\n",
    "        gdf_nuts_geom,\n",
    "        style_function=lambda feature: {\n",
    "            'fillColor': feature['properties']['rgba'],\n",
    "            'color' : feature['properties']['rgba'],\n",
    "            'weight' : 1,\n",
    "            'fillOpacity' : 0.7,\n",
    "            },\n",
    "        tooltip=folium.GeoJsonTooltip(\n",
    "            fields=['NUTS3','cluster_id'],\n",
    "            aliases=['Provincia','Cluster'],\n",
    "            localize=True\n",
    "        )\n",
    "\n",
    "        ).add_to(m)\n",
    "\n",
    "    # folium.map.CustomPane('labels').add_to(m)\n",
    "#     folium.TileLayer('CartoDBPositronOnlyLabels',\n",
    "#                      pane='labels').add_to(m)\n",
    "    #m.save('clustering/clustering_{}.html'.format(name))\n",
    "\n",
    "    return m,gb_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R2Lrk5d3lt76",
    "outputId": "f5a09569-4e44-45da-e551-e28f2024f215"
   },
   "outputs": [],
   "source": [
    "m,clustered = clustering(gb_df,clustering_columns_mean_1,province_shape,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9R6cItQnlt_C"
   },
   "outputs": [],
   "source": [
    "clustering_columns_mean_1 = [\n",
    "              'mean_best_practices',\n",
    "              'mean_seo',\n",
    "              'mean_ratio_links_in',\n",
    "              'mean_ratio_links_out',\n",
    "              'mean_years_old',\n",
    "              'mean_security_header_int',\n",
    "              'mean_request_time',\n",
    "              'mean_length_url',\n",
    "              'percentage_facebook',\n",
    "              'percentage_instagram',\n",
    "              'percentage_linkedin']\n",
    "\n",
    "clustering_columns_mean_2 = [\n",
    "                           'mean_bad_images',\n",
    "                           'mean_ratio_links',\n",
    "                           'mean_ratio_links_in',\n",
    "                           'mean_ratio_links_out',\n",
    "                           'mean_years_old',\n",
    "                           'mean_security_header_int',\n",
    "                           'mean_text_len',\n",
    "                           'mean_request_time',\n",
    "                           'mean_length_url',\n",
    "                           'percentage_facebook',\n",
    "                           'percentage_instagram',\n",
    "                           'percentage_linkedin'\n",
    "                          ]\n",
    "clustering_columns_median_1 = [\n",
    "                             'median_request_time',\n",
    "                             'median_images',\n",
    "                             'median_links',\n",
    "                             'median_links_in',\n",
    "                             'median_links_out',\n",
    "                             'median_years_old',\n",
    "                             'median_ratio_links',\n",
    "                             'median_ratio_links_in',\n",
    "                             'median_ratio_links_out',\n",
    "                             'median_bad_images',\n",
    "                             'median_security_header_int',\n",
    "                             'median_text_len',\n",
    "                             'median_length_url',\n",
    "                             'percentage_facebook',\n",
    "                             'percentage_instagram',\n",
    "                             'percentage_linkedin'\n",
    "                            ]\n",
    "clustering_columns_median_2 = [\n",
    "                             'median_request_time',\n",
    "                             'median_years_old',\n",
    "                             'median_ratio_links',\n",
    "                             'median_ratio_links_in',\n",
    "                             'median_ratio_links_out',\n",
    "                             'median_bad_images',\n",
    "                             'median_security_header_int',\n",
    "                             'median_text_len',\n",
    "                             'median_length_url',\n",
    "                             'percentage_facebook',\n",
    "                             'percentage_instagram',\n",
    "                             'percentage_linkedin'\n",
    "                            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LL91PkQPluCA"
   },
   "outputs": [],
   "source": [
    "clustering_tests = {\n",
    "    'mean_1':clustering_columns_mean_1,\n",
    "    'mean_2':clustering_columns_mean_2,\n",
    "    'median_1':clustering_columns_median_1,\n",
    "    'median_2':clustering_columns_median_2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KFRnTIJgluFf",
    "outputId": "f443507b-d491-4d69-bddb-de1d0ad00e7a"
   },
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "for n in clustering_tests.keys():\n",
    "    test_results[n] = clustering(gb_df,clustering_tests[n],province_shape,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "kg1oRNJjoscr",
    "outputId": "f267f9ed-dd9b-41cd-cbe8-5b33d5110b09"
   },
   "outputs": [],
   "source": [
    "test_results[\"mean_1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CvU280On9xb"
   },
   "outputs": [],
   "source": [
    "\n",
    "for k in test_results.keys():\n",
    "  test_results[k][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4N-qrYrn9CJ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We can repeat the process at different spatial granularity level --> municipalities\n",
    "'''\n",
    "\n",
    "comuni_shape = gpd.read_file('/content/drive/Shareddrives/phd_hands_on/clustering_test/Com01012021_g_WGS84.shp',encoding='utf-8')[['COMUNE','geometry']]\n",
    "\n",
    "comuni_shape['lower'] = comuni_shape['COMUNE'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZSulhV2n8NL"
   },
   "outputs": [],
   "source": [
    "gb_df = df_full[df_full.NUTS_ID != 'ITF48'].groupby(['Città\\nLocal Alphabet'],as_index=False).agg(\n",
    "        NUTS3 = pd.NamedAgg(column='NUTS3', aggfunc='first'),\n",
    "        total = pd.NamedAgg(column='Codicefiscale', aggfunc='count'),\n",
    "        instagrams = pd.NamedAgg(column='instagram', aggfunc='sum'),\n",
    "        linkedins = pd.NamedAgg(column='linkedin', aggfunc='sum'),\n",
    "        facebooks = pd.NamedAgg(column='facebook', aggfunc='sum'),\n",
    "\n",
    "        mean_images = pd.NamedAgg(column='n_images', aggfunc='mean'),\n",
    "        mean_bad_images = pd.NamedAgg(column='bad_images_ratio', aggfunc='mean'),\n",
    "        mean_links = pd.NamedAgg(column='n_links', aggfunc='mean'),\n",
    "        mean_links_in = pd.NamedAgg(column='links_in', aggfunc='mean'),\n",
    "        mean_links_out = pd.NamedAgg(column='links_out', aggfunc='mean'),\n",
    "        mean_ratio_links = pd.NamedAgg(column='ratio_links', aggfunc='mean'),\n",
    "        mean_ratio_links_in = pd.NamedAgg(column='ratio_links_in', aggfunc='mean'),\n",
    "        mean_ratio_links_out = pd.NamedAgg(column='ratio_links_out', aggfunc='mean'),\n",
    "        mean_years_old = pd.NamedAgg(column='years_old', aggfunc='mean'),\n",
    "        mean_security_header_int = pd.NamedAgg(column='security_header_int', aggfunc='mean'),\n",
    "        mean_text_len = pd.NamedAgg(column='text_len', aggfunc='mean'),\n",
    "        mean_request_time = pd.NamedAgg(column='request_time', aggfunc='mean'),\n",
    "        mean_length_url = pd.NamedAgg(column='length_url', aggfunc='mean'),\n",
    "        mean_accessibility = pd.NamedAgg(column='accessibility', aggfunc='mean'),\n",
    "        mean_best_practices = pd.NamedAgg(column='best-practices', aggfunc='mean'),\n",
    "        mean_performance = pd.NamedAgg(column='performance', aggfunc='mean'),\n",
    "        mean_seo = pd.NamedAgg(column='seo', aggfunc='mean'),\n",
    "\n",
    "        median_images = pd.NamedAgg(column='n_images', aggfunc='median'),\n",
    "        median_request_time = pd.NamedAgg(column='request_time', aggfunc='median'),\n",
    "        median_links = pd.NamedAgg(column='n_links', aggfunc='median'),\n",
    "        median_links_in = pd.NamedAgg(column='links_in', aggfunc='median'),\n",
    "        median_links_out = pd.NamedAgg(column='links_out', aggfunc='median'),\n",
    "        median_years_old = pd.NamedAgg(column='years_old', aggfunc='median'),\n",
    "        median_ratio_links = pd.NamedAgg(column='ratio_links', aggfunc='median'),\n",
    "        median_ratio_links_in = pd.NamedAgg(column='ratio_links_in', aggfunc='median'),\n",
    "        median_ratio_links_out = pd.NamedAgg(column='ratio_links_out', aggfunc='median'),\n",
    "        median_bad_images = pd.NamedAgg(column='bad_images_ratio', aggfunc='median'),\n",
    "        median_security_header_int = pd.NamedAgg(column='security_header_int', aggfunc='median'),\n",
    "        median_text_len = pd.NamedAgg(column='text_len', aggfunc='median'),\n",
    "        median_length_url = pd.NamedAgg(column='length_url', aggfunc='median'),\n",
    "\n",
    "        median_accessibility = pd.NamedAgg(column='accessibility', aggfunc='median'),\n",
    "        median_best_practices = pd.NamedAgg(column='best-practices', aggfunc='median'),\n",
    "        median_performance = pd.NamedAgg(column='performance', aggfunc='median'),\n",
    "        median_seo = pd.NamedAgg(column='seo', aggfunc='median')\n",
    "    )\n",
    "\n",
    "gb_df['percentage_facebook'] = gb_df.facebooks/gb_df.total*100\n",
    "gb_df['percentage_instagram'] = gb_df.instagrams/gb_df.total*100\n",
    "gb_df['percentage_linkedin'] = gb_df.linkedins/gb_df.total*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsPJ6pOvq_Cg"
   },
   "outputs": [],
   "source": [
    "\n",
    "gb_df['lower'] = gb_df['Città\\nLocal Alphabet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGAS2e9srH7R"
   },
   "outputs": [],
   "source": [
    "\n",
    "gb_df = gb_df[gb_df.total >= 10].copy()\n",
    "gb_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LInOQGiqrQyk"
   },
   "outputs": [],
   "source": [
    "\n",
    "clustering_columns_mean_1 = [\n",
    " 'mean_best_practices',\n",
    " 'mean_seo',\n",
    " 'mean_ratio_links_in',\n",
    " 'mean_ratio_links_out',\n",
    " 'mean_years_old',\n",
    " 'mean_security_header_int',\n",
    " 'mean_request_time',\n",
    " 'mean_length_url',\n",
    " 'percentage_facebook',\n",
    " 'percentage_instagram',\n",
    " 'percentage_linkedin']\n",
    "\n",
    "columns_to_scale_mean_1 = [\n",
    " 'mean_ratio_links_in',\n",
    " 'mean_ratio_links_out',\n",
    " 'mean_years_old',\n",
    " 'mean_security_header_int',\n",
    " 'mean_request_time',\n",
    " 'mean_length_url',\n",
    " 'percentage_facebook',\n",
    " 'percentage_instagram',\n",
    " 'percentage_linkedin','mean_best_practices',\n",
    " 'mean_seo',\n",
    "]\n",
    "columns_not_scaled = [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "htRxagINrc0p"
   },
   "outputs": [],
   "source": [
    "gb_df.columns = [c.replace('-','_') for c in gb_df.columns]\n",
    "\n",
    "columns = clustering_columns_mean_1\n",
    "columns_to_scale = columns_to_scale_mean_1\n",
    "df_clustering = gb_df[columns].copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df_clustering[columns_to_scale])\n",
    "\n",
    "df_scaled = pd.DataFrame(scaled,columns=columns_to_scale)\n",
    "df_scaled = pd.merge(df_scaled,df_clustering[columns_not_scaled],left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "lDx_V_eJrcyi",
    "outputId": "cbaf9fd5-f43c-47c0-d1ce-9f38eb209213"
   },
   "outputs": [],
   "source": [
    "columns = clustering_columns_mean_1\n",
    "df_clustering = gb_df[columns].copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df_clustering)\n",
    "\n",
    "df_scaled = pd.DataFrame(scaled,columns=columns)\n",
    "\n",
    "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(df_scaled)\n",
    "                for k in range(1, 10)]\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(range(1, 10), inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.annotate('Elbow',\n",
    "             xy=(3, inertias[2]),\n",
    "             xytext=(0.55, 0.55),\n",
    "             textcoords='figure fraction',\n",
    "             fontsize=16,\n",
    "             arrowprops=dict(facecolor='black', shrink=0.1)\n",
    "            )\n",
    "# plt.axis([1, 8.5, 0, 200])\n",
    "#     plt.savefig('clustering/elbow_{}.png'.format(name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WWlf56HgrcvC",
    "outputId": "a02b5f09-d893-4fa6-abae-77192cc6d666"
   },
   "outputs": [],
   "source": [
    "km = kmeans_per_k[2]\n",
    "\n",
    "gb_df['cluster_id'] = km.labels_\n",
    "\n",
    "print(gb_df.cluster_id.value_counts())\n",
    "\n",
    "gb_df.columns = [c.replace('_','-') if c in columns else c for c in gb_df.columns]\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "\n",
    "for i,c in enumerate(km.cluster_centers_):\n",
    "    plt.plot(c,label='cluster {}'.format(i),marker='x',linewidth=3,color=get_color(i))\n",
    "\n",
    "ticks = list(range(0,len(columns),1))\n",
    "columns = [c.replace('_','-') for c in columns]\n",
    "plt_labels = columns[::1]\n",
    "plt.xticks(fontsize=24)\n",
    "plt.yticks(fontsize=24)\n",
    "\n",
    "plt.ylabel('Mean value', fontsize=32)\n",
    "plt.xlabel('Clustering features', fontsize=32)\n",
    "\n",
    "plt.grid()\n",
    "plt.xticks(ticks=ticks,labels=plt_labels,rotation = 90)\n",
    "plt.legend(loc='best',fontsize=24)\n",
    "# plt.savefig('clustering/centroids_{}.png'.format(name))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GaK0sjrorcry"
   },
   "outputs": [],
   "source": [
    "gb_df['lower'] = gb_df['Città\\nLocal Alphabet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mvhJVCprcpJ"
   },
   "outputs": [],
   "source": [
    "df_nuts_geom = pd.merge(gb_df,comuni_shape,on='lower')\n",
    "\n",
    "df_nuts_geom['rgba'] = df_nuts_geom.cluster_id.apply(lambda x: get_color(x).lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MgnDZNutrcl5",
    "outputId": "33c31697-b3ca-4a03-a62d-6a8fb7d53f49"
   },
   "outputs": [],
   "source": [
    "# clusters = [2]\n",
    "gdf_nuts_geom = gpd.GeoDataFrame(df_nuts_geom,geometry='geometry')\n",
    "gdf_nuts_geom.dropna(inplace=True)\n",
    "# gdf_nuts_geom = gdf_nuts_geom[gdf_nuts_geom.cluster_id.isin(clusters)]\n",
    "m = folium.Map((41.9027835,12.4963655),zoom_start=6)\n",
    "folium.GeoJson(\n",
    "    gdf_nuts_geom,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': feature['properties']['rgba'],\n",
    "        'color' : feature['properties']['rgba'],\n",
    "        'weight' : 0.7,\n",
    "        'fillOpacity' : 0.5,\n",
    "        },\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=['lower','cluster_id','total'],\n",
    "        aliases=['Comune','Cluster','Totale imprese'],\n",
    "        localize=True\n",
    "    )\n",
    "\n",
    "    ).add_to(m)\n",
    "\n",
    "# folium.map.CustomPane('labels').add_to(m)\n",
    "# #     folium.TileLayer('CartoDBPositronOnlyLabels',\n",
    "# #                      pane='labels').add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "vHI9tisarcfn",
    "outputId": "f15bed16-b383-456b-810c-b5c589f86f7c"
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdQDGAJjrcSA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dujye_h8y7Yw"
   },
   "source": [
    "## DBSCAN Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "It is a density-based clustering non-parametric algorithm (you don't need to specify the number of $k$ clusters to extract): given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.\n",
    "\n",
    "How it works:\n",
    "\n",
    "1. For each instance it counts how many instances are located withing a small distance **$\\epsilon$** from it. The region is called the instance's **$\\epsilon$-neighborhood**\n",
    "2. If an instance has at least **min_sample** instances in its **$\\epsilon$-neighborhood**, then it is considered a *core instance*\n",
    "3. All the instances in the neighborhood of a core instance belong to the same cluster\n",
    "4. Any instance that is not a core instance and does not have one in its neighborhood is considered as **anomaly** (noise)\n",
    "\n",
    "\n",
    "```python\n",
    "class sklearn.cluster.DBSCAN(eps=0.5, *, min_samples=5, metric='euclidean',\n",
    "metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None)\n",
    "```\n",
    "\n",
    "Further details [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8LQkYvDYvwK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "ZaDG_QGyGk3-",
    "outputId": "6dffb2f8-bcf4-4e85-f21a-8c28f61c9464"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "'''\n",
    "two main parameters: eps and min_samples.\n",
    "'''\n",
    "\n",
    "dbscan = DBSCAN(eps=0.45, min_samples=5)\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAsHehhSGkyC"
   },
   "outputs": [],
   "source": [
    "labels = pd.Series(dbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yd5Yd3BTQWdA",
    "outputId": "1421884d-864b-498b-80d2-248690b70829"
   },
   "outputs": [],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "rAFaEaEPGkr4",
    "outputId": "384780e4-ffba-4701-9823-f1a733612feb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "which are the labels of our clusters?\n",
    "'''\n",
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yum-1gNw0v7I"
   },
   "source": [
    "### How can we find the best value, for instance, for $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u0MvxjTYGkm3"
   },
   "outputs": [],
   "source": [
    "#run of dbscan with different eps parameter\n",
    "\n",
    "dbscan_per_eps = [DBSCAN(eps=eps, min_samples=5).fit(X)\n",
    "                for eps in np.arange(0.1,0.5,0.05)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWjixoNwGkW1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "also for dbscan we can compute the silhouette.\n",
    "'''\n",
    "silhouette_scores_dbs = [silhouette_score(X, model.labels_)\n",
    "                     for model in dbscan_per_eps if len(set(model.labels_)) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "9Y7Thc_VKgi7",
    "outputId": "18dc62bd-536a-4ac7-f8d9-2952da17af35"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Silhouette plots also for dbscan\n",
    "'''\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "import math\n",
    "\n",
    "plt.figure(figsize=(11, 9))\n",
    "\n",
    "for i, eps in enumerate(np.arange(0.1,0.5,0.05)):\n",
    "    y_pred = dbscan_per_eps[i].labels_\n",
    "    if len(set(y_pred))>1:\n",
    "\n",
    "      n_cols = 2\n",
    "      n_rows = math.ceil(len(dbscan_per_eps)/n_cols)\n",
    "      plt.subplot(n_rows, n_cols, i+1)\n",
    "\n",
    "      silhouette_coefficients = silhouette_samples(X, y_pred)\n",
    "\n",
    "      padding = len(X) // 10\n",
    "      pos = padding\n",
    "      ticks = []\n",
    "      for j in [-1]+list(range(i)):\n",
    "          coeffs = silhouette_coefficients[y_pred == j]\n",
    "          coeffs.sort()\n",
    "\n",
    "          color = mpl.cm.Spectral((j+1) / (i))\n",
    "          plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                            facecolor=color, edgecolor=color, alpha=0.7)\n",
    "          ticks.append(pos + len(coeffs) // 2)\n",
    "          pos += len(coeffs) + padding\n",
    "\n",
    "      plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
    "      plt.gca().yaxis.set_major_formatter(FixedFormatter([-1]+list(range(i))))\n",
    "      if i in (2, 4, 6):\n",
    "          plt.ylabel(\"Cluster\")\n",
    "\n",
    "      if i in (6, 7):\n",
    "          # plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "          plt.xlabel(\"Silhouette Coefficient\")\n",
    "      else:\n",
    "          plt.tick_params(labelbottom=False)\n",
    "\n",
    "      plt.axvline(x=silhouette_scores[i], color=\"red\", linestyle=\"--\")\n",
    "      plt.title(\"$k={:.2f}$\".format(eps), fontsize=16)\n",
    "\n",
    "# save_fig(\"silhouette_analysis_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pUXt592Yzyz"
   },
   "source": [
    "The algorithm is based on regions with higher density, such as regions where the distance betweeen the objects is smaller.\n",
    "\n",
    "Therefore, another way to properly set a good value for $\\epsilon$ is to look at the distribution of the distances of closest neighboors.  \n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iaMa2nmakn21"
   },
   "outputs": [],
   "source": [
    "# we take the distance between the 2 closest points for each object\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "FmakPLy4knxX",
    "outputId": "61b2e657-1ea4-442c-b550-3a6b81984d2e"
   },
   "outputs": [],
   "source": [
    "# plot sorted distances and find where there is a elbow (again)\n",
    "# in this case is aro\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances_plot = distances[:,1]\n",
    "plt.plot(distances_plot)\n",
    "plt.hlines(0.4,-40,180,linestyles='--',colors='red')\n",
    "plt.hlines(0.5,-40,180,linestyles='--',colors='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "3PLgJC9LlNpu",
    "outputId": "c6ea8ffa-ebef-4db5-b271-0f31e528e5cf"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.45,min_samples=5)\n",
    "dbscan.fit(X)\n",
    "clusters = dbscan.labels_\n",
    "\n",
    "X['cluster_id'] = dbscan.labels_\n",
    "figure, axes = plt.subplots(nrows=1,ncols=2,sharey=True)\n",
    "sns.scatterplot(data=X,x=X['petal length (cm)'],y=X['petal width (cm)'], hue='cluster_id', palette='tab10',ax=axes[0])\n",
    "core_points = X[X.index.isin(dbscan.core_sample_indices_)]\n",
    "sns.scatterplot(data=core_points,x=core_points['petal length (cm)'],y=core_points['petal width (cm)'],size=20,markers=['x'],color='black',ax=axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_torPMAy7Y0"
   },
   "source": [
    "## Other Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9rHhuezy7Y1"
   },
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "O2F4hOl3255S",
    "outputId": "a2ea8ee7-c18a-47c1-dcb0-dc41a88e0822"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "model = AgglomerativeClustering(distance_threshold=0, n_clusters=None)\n",
    "\n",
    "model = model.fit(X)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(model, truncate_mode='level', p=3)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXSZr1dzy7Y0"
   },
   "source": [
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3MkbWb4y7Y0"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "08gHIw-Dy7Y0",
    "outputId": "4b2f90d4-7049-441b-e48a-61d9758bf7ae"
   },
   "outputs": [],
   "source": [
    "sc1 = SpectralClustering(n_clusters=2, gamma=100, random_state=42)\n",
    "sc1.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "CYubCAWFy7Y0",
    "outputId": "9a84a8b6-d32c-4817-8b3f-fccce209802e"
   },
   "outputs": [],
   "source": [
    "sc2 = SpectralClustering(n_clusters=2, gamma=1, random_state=42)\n",
    "sc2.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9WyYgFy_y7Y0",
    "outputId": "9f412859-e918-43bd-fdf7-08b318a3a452"
   },
   "outputs": [],
   "source": [
    "np.percentile(sc1.affinity_matrix_, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PturCjKQy7Y0"
   },
   "outputs": [],
   "source": [
    "def plot_spectral_clustering(sc, X, size, alpha, show_xlabels=True, show_ylabels=True):\n",
    "    plt.scatter(X[:, 0], X[:, 1], marker='o', s=size, c='gray', cmap=\"Paired\", alpha=alpha)\n",
    "    plt.scatter(X[:, 0], X[:, 1], marker='o', s=30, c='w')\n",
    "    plt.scatter(X[:, 0], X[:, 1], marker='.', s=10, c=sc.labels_, cmap=\"Paired\")\n",
    "\n",
    "    if show_xlabels:\n",
    "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
    "    else:\n",
    "        plt.tick_params(labelbottom=False)\n",
    "    if show_ylabels:\n",
    "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
    "    else:\n",
    "        plt.tick_params(labelleft=False)\n",
    "    plt.title(\"RBF gamma={}\".format(sc.gamma), fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "zs4V7S-_y7Y1",
    "outputId": "a460df9f-ffa8-496c-cdcc-dce85d172d6b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3.2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_spectral_clustering(sc1, X, size=500, alpha=0.1)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_spectral_clustering(sc2, X, size=4000, alpha=0.01, show_ylabels=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aCkEl668vXJg"
   },
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "- Use a dimensionality reduction algorithm to speed-up the application of a clustering algorithm obtaining similar results\n",
    "\n",
    "## *Principal Component Analysis (PCA)*\n",
    "\n",
    "``` python\n",
    "class sklearn.decomposition.PCA(n_components=None, *, copy=True,\n",
    "whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto',\n",
    "random_state=None)\n",
    "```\n",
    "\n",
    "- Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD\n",
    "\n",
    "[PCA scikit-learn page](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deXGIigQK0mT"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nBts8PuF8hk"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzTPVKMMSwcY",
    "outputId": "e0243d1c-124c-4c43-acbe-768bb653719a"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TDIuHQqF-Ri"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "#find the number of components needed to cover the 0.95 of the variance\n",
    "\n",
    "d = np.argmax(cumsum >= 0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWhdIsFnLsHv",
    "outputId": "476ec72a-7e96-4d2a-9ef1-0899502f5fa2"
   },
   "outputs": [],
   "source": [
    "print('How many components do we need to have 95% of the variance? {}'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "v2cZd6HjJ5R9",
    "outputId": "d8e0622a-4121-47ac-8874-d62004be1f18"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "plt.axis([0, 400, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")\n",
    "plt.plot(d, 0.95, \"ko\")\n",
    "plt.annotate(\"Elbow\", xy=(65, 0.85), xytext=(70, 0.7),\n",
    "             arrowprops=dict(arrowstyle=\"->\"), fontsize=16)\n",
    "plt.grid(True)\n",
    "# save_fig(\"explained_variance_plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "4kyKAXTGMhjb",
    "outputId": "fafebc9b-f4d7-4edf-eaf3-af2e6f1f1e1a"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "the n_components parameter can take values with different semantics:\n",
    "- an integer that corresponds to the number of components\n",
    "- a float in the interval (0,1) as the explained variance required and the number\n",
    "of components it is determined automatically by the PCA class\n",
    "'''\n",
    "\n",
    "# example of the first use\n",
    "pca = PCA(n_components=0.99)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "print('How many components? '.format(pca.n_components_))\n",
    "\n",
    "print('Explained variance {:.2f}'.format(np.sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "# example of the second use\n",
    "pca = PCA(n_components=331)\n",
    "\n",
    "#remember scikit transform, fit, and inverse_transform\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n",
    "\n",
    "# save_fig(\"mnist_compression_plot\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A5aLq7UDQoX7",
    "outputId": "fb7a8505-79a6-4f57-9865-90e4b3903976"
   },
   "outputs": [],
   "source": [
    "X_reduced_pca = X_reduced\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_reduced = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans_reduced.fit(X_reduced_pca)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "def plot_clusters(faces, labels, n_cols=5):\n",
    "#     inversed = pca.inverse_transform(faces)\n",
    "#     print(inversed.shape)\n",
    "    faces = faces.reshape(-1,28, 28)\n",
    "    n_rows = (len(faces) - 1) // n_cols + 1\n",
    "    plt.figure(figsize=(n_cols, n_rows * 1.1))\n",
    "    for index, (face, label) in enumerate(zip(faces, labels)):\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(face, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(label)\n",
    "    plt.show()\n",
    "\n",
    "for cluster_id in (set(kmeans_reduced.labels_)):\n",
    "    print(\"Cluster\", cluster_id)\n",
    "    in_cluster = kmeans_reduced.labels_==cluster_id\n",
    "    digits = X_train[in_cluster]\n",
    "    labels = y_train[in_cluster]\n",
    "    plot_clusters(digits[:10,:], labels[:10])\n",
    "\n",
    "\n",
    "## How can I compute the most relevant label for each cluster?\n",
    "## add a column with the right label and the cluster_id\n",
    "## compute the value counts or use the groupby function (we didn't see it)\n",
    "\n",
    "## how we can compute the differences between the results obtained on the reduced\n",
    "## w.r.t. the real ones?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-u0YO01VAKaN",
    "outputId": "1b494319-5ff4-4c81-89ef-a8d3b6e6ab71"
   },
   "outputs": [],
   "source": [
    "for cluster_id in (set(kmeans.labels_)):\n",
    "    print(\"Cluster\", cluster_id)\n",
    "    in_cluster = kmeans.labels_==cluster_id\n",
    "    digits = X_train[in_cluster]\n",
    "    labels = y_train[in_cluster]\n",
    "    plot_clusters(digits[:10,:], labels[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2jrsTFkQpXZ"
   },
   "source": [
    "- It is possible to speed-up the process setting the ```svd_solver``` parameter to ```randomized```.\n",
    "\n",
    "- It uses a stochastic algorithm that quickly finds an approximation of the first *d* principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "_39GNYSJMu8r",
    "outputId": "9aa274e7-5c3d-4e63-eec6-4b82110d302c"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=311,svd_solver=\"randomized\",random_state=42)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "print('Number of components {}'.format(pca.n_components_))\n",
    "\n",
    "print('The total explained variance is {:.2f}'.format(np.sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Compressed\", fontsize=16)\n",
    "\n",
    "# save_fig(\"mnist_compression_plot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Phi8UyABXWk"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lecture_2_Unsupervised_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
