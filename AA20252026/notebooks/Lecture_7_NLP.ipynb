{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QNW8f3BPIoi"
      },
      "source": [
        "# Machine Learning for Software Analysis (MLSA)\n",
        "\n",
        "## University of Florence -- IMT School for Advanced Studies Lucca\n",
        "\n",
        "### Fabio Pinelli\n",
        "<a href=\"mailto:fabio.pinelli@imtlucca.it\">fabio.pinelli@imtlucca.it</a><br/>\n",
        "IMT School for Advanced Studies Lucca<br/>\n",
        "2025/2026<br/>\n",
        "November, 11 2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8BhclloPIom"
      },
      "outputs": [],
      "source": [
        "###########################################################\n",
        "!pip install gensim==4.3.3\n",
        "# The library has been archived and won't be used anymore\n",
        "# # !pip install allennlp==0.9.0\n",
        "!pip install flair==0.13.1\n",
        "!pip install torchvision==0.18.1\n",
        "# # HuggingFace\n",
        "\n",
        "!pip uninstall -y transformers peft\n",
        "!pip install transformers==4.38.0\n",
        "!pip install datasets==2.18.0\n",
        "!pip install peft==0.8.2\n",
        "!pip install accelerate==0.30.0\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5okKRDWPIon"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter11()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter11 import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMmcGqLxPIoo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import errno\n",
        "import requests\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from operator import itemgetter\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "from data_generation.nlp import ALICE_URL, WIZARD_URL, download_text\n",
        "from stepbystep.v4 import StepByStep\n",
        "# These are the classes we built in previous class 10\n",
        "from seq2seq import *\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU1AMangTB1o"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim import corpora, downloader\n",
        "from gensim.parsing.preprocessing import *\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6HR2OpPTtEJ"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "#from flair.embeddings import ELMoEmbeddings, WordEmbeddings, \\\n",
        "#    TransformerWordEmbeddings, TransformerDocumentEmbeddings\n",
        "from flair.embeddings import WordEmbeddings, \\\n",
        "    TransformerWordEmbeddings, TransformerDocumentEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfXyVryWT2mM"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Split\n",
        "from transformers import (\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BertModel, BertTokenizer, BertForSequenceClassification,\n",
        "    DistilBertModel, DistilBertTokenizer,\n",
        "    DistilBertForSequenceClassification,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModel, AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, pipeline, TextClassificationPipeline\n",
        ")\n",
        "from transformers.pipelines import SUPPORTED_TASKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOMOKCI3PIop"
      },
      "source": [
        "# Down the Yellow Brick Rabbit Hole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG2bSGFtPIop"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/alice_dorothy.png?raw=1)\n",
        "\n",
        "*Left: \"Alice and the Baby Pig\" illustration by John Tenniel's, from \"Alice's Adventure's in Wonderland\" (1865).*\n",
        "\n",
        "*Right: \"Dorothy meets the Cowardly Lion\" illustration by W.W. Denslow, from \"The Wonderful Wizard of Oz\" (1900)*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification task\n",
        "\n",
        "- Given a sentence we want to classify if this is written into *ALICE'S ADVENTURES IN WONDERLAN* or *The wonderful Wizard of Oz*.\n",
        "\n",
        "- This is a Natural Language Processing (NLP) task.\n",
        "\n",
        "- We explore how to handle word that are not numbers. How we can use them to learn and train a model and use it in various tasks.\n",
        "\n",
        "- Tokenization, Embeddings, Bert, ChatGPT...\n",
        "\n",
        "- Some references to understand what we do in Software Analysis with ML (you can find them also on our repository)\n",
        "\n",
        "https://arxiv.org/pdf/2002.08155\n",
        "\n",
        "\n",
        "https://arxiv.org/abs/1803.09473\n",
        "\n",
        "\n",
        "https://dl.acm.org/doi/pdf/10.1145/3460348"
      ],
      "metadata": {
        "id": "lINat3vNKZQP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqGeN0zzPIoq"
      },
      "source": [
        "# Building a Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIiNrb9uPIor"
      },
      "outputs": [],
      "source": [
        "localfolder = 'texts'\n",
        "download_text(ALICE_URL, localfolder)\n",
        "download_text(WIZARD_URL, localfolder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IceFMuyPIor"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(localfolder, 'alice28-1476.txt'), 'r') as f:\n",
        "    alice = ''.join(f.readlines()[104:3704])\n",
        "\n",
        "with open(os.path.join(localfolder, 'wizoz10-1740.txt'), 'r') as f:\n",
        "    wizard = ''.join(f.readlines()[310:5100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hpw53y4yPIos"
      },
      "outputs": [],
      "source": [
        "print(alice[:500])\n",
        "print('\\n')\n",
        "print(wizard[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can explore the content of the downloaded text on Colab left panel [folder icon] and you could see that the initial part of both books contains useless text for our purposes.\n",
        "\n",
        "We write a cfg file that contains the lines to be considered then we will proceed on identifying sentences in the books that will be our training dataset"
      ],
      "metadata": {
        "id": "TIue_H2tMJkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNdQPUzqPIos"
      },
      "outputs": [],
      "source": [
        "text_cfg = \"\"\"fname,start,end\n",
        "alice28-1476.txt,104,3704\n",
        "wizoz10-1740.txt,310,5100\"\"\"\n",
        "bytes_written = open(os.path.join(localfolder, 'lines.cfg'), 'w').write(text_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting the goals\n",
        "\n",
        "Our goal is to **transform** the books in CSV files so that they can be read as datasets.\n",
        "\n",
        "Each line of the CSV will have a line for each sentence of the book, so that we could try to classify them, accordingly.\n",
        "\n",
        "Something like:\n",
        "```\n",
        "sentence,source\n",
        "\"dsf sdf sdfsd sdfs, sdfs: dsfsdf, sdfs\", Alice\n",
        "\"ipopsd iops siopiopi sdoiop sdifop\", Alice\n",
        "\"mnmnb nmmnnj jj kjkj; \\\"jkj jk jk, \", Alice\n",
        "\"qweq qweq qweqw, qweqe ,\\\"qweqwe w,qwe,w\\\" \", Alice\n",
        "```"
      ],
      "metadata": {
        "id": "SpbrvGgARmBv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd4tQSexPIot"
      },
      "source": [
        "## Sentence Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might have already met the words **token** and **tokenization**\n",
        "- A **token** is a piece of text\n",
        "- Tokenize a text means to split it into pieces (token), getting as result a list of **tokens**\n",
        "\n",
        "The most common kind of pieces is a word, typically tokenizing a text usually means to split it into words, for instance, using the **white space** a separator."
      ],
      "metadata": {
        "id": "IfflOI92NrnX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms_jPDTWPIot"
      },
      "outputs": [],
      "source": [
        "sentence = \"I'm following the white rabbit\"\n",
        "tokens = sentence.split(' ')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about *I'm*, it should be two tokens or just one?\n",
        "\n",
        "IT DEPENDS :-)!\n",
        "\n",
        "We will see later how to more properly perform tokenization at the word level.\n",
        "\n",
        "For now, we are interested in sentence tokenization, our task is to classify sentences.\n",
        "\n",
        "In order to do that, we use a library called NLTK (Natural Language Toolkit) and it ```sent_tokenize()``` method.\n"
      ],
      "metadata": {
        "id": "dQvthAoKOgKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```punkt``` is a pre-trained model for tokenization.\n",
        "\n",
        "- It's based on the Punkt Sentence Tokenizer, a popular unsupervised method for sentence boundary detection.\n",
        "- NLTK's punkt tokenizer can split text into sentences and further tokenize sentences into words.\n",
        "- Language Support: punkt includes models trained for multiple languages (e.g., English, German, French)."
      ],
      "metadata": {
        "id": "pkLxLwxpPtMy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxbboLMMPIou"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "corpus_alice = sent_tokenize(alice)\n",
        "corpus_wizard = sent_tokenize(wizard)\n",
        "len(corpus_alice), len(corpus_wizard)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **corpus** is a structured set of **Documents**.\n",
        "\n",
        "A document can be a **sentence**, a **paragraph**, a **tweet** or a **book**.\n",
        "\n",
        "In our case, the document is a **sentence**, so each book is actually a set of **sentences**, thus each book may be considered a corpus...\n",
        "\n",
        "Its plural???\n"
      ],
      "metadata": {
        "id": "dolXri9-QdA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CORPORA**"
      ],
      "metadata": {
        "id": "TakCC8cvRHQb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ0TkRtRPIou"
      },
      "outputs": [],
      "source": [
        "corpus_alice[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This includes ```\\n```, the sentence tokenizer only handles the sentence splitting and it doesn't clean the text."
      ],
      "metadata": {
        "id": "x5X_9k6HRM-m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFOXRwwLPIov"
      },
      "outputs": [],
      "source": [
        "corpus_wizard[30]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same here, we have \" the quotation mark"
      ],
      "metadata": {
        "id": "yyo8zAyBRaz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, we need to create CSV files with one sentence per line. Therefore, we need to:\n",
        "- Clean the line breaks\n",
        "- Define a quote char to wrap such that the original commas or semicolons are not misinteoreted as separation chars of the CSV\n",
        "- add a second column (label) to identify the source\n",
        "\n",
        "Then we will concatenate everything, shuffle the sentences before training a model\n",
        "\n",
        "We expect to have something like this:\n",
        "```\n",
        "\\\"There's a cyclone coming, Em,\" he called to his wife.\\,wizoz10-1740.txt\n",
        "```\n",
        "Where the **escape** character \"**\\\\**\" is the quote char because is not present in any (novel) books.\n",
        "\n",
        "Different choices has to be done when we will deal with code data."
      ],
      "metadata": {
        "id": "TiUQQLWgSBqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TX0L1NqPIov"
      },
      "outputs": [],
      "source": [
        "def sentence_tokenize(source, quote_char='\\\\', sep_char=',',\n",
        "                      include_header=True, include_source=True,\n",
        "                      extensions=('txt'), **kwargs):\n",
        "    nltk.download('punkt')\n",
        "    # If source is a folder, goes through all files inside it\n",
        "    # that match the desired extensions ('txt' by default)\n",
        "    if os.path.isdir(source):\n",
        "        filenames = [f for f in os.listdir(source)\n",
        "                     if os.path.isfile(os.path.join(source, f)) and\n",
        "                        os.path.splitext(f)[1][1:] in extensions]\n",
        "    elif isinstance(source, str):\n",
        "        filenames = [source]\n",
        "\n",
        "    # If there is a configuration file, builds a dictionary with\n",
        "    # the corresponding start and end lines of each text file\n",
        "    config_file = os.path.join(source, 'lines.cfg')\n",
        "    config = {}\n",
        "    if os.path.exists(config_file):\n",
        "        with open(config_file, 'r') as f:\n",
        "            rows = f.readlines()\n",
        "\n",
        "        for r in rows[1:]:\n",
        "            fname, start, end = r.strip().split(',')\n",
        "            config.update({fname: (int(start), int(end))})\n",
        "\n",
        "    new_fnames = []\n",
        "    # For each file of text\n",
        "    for fname in filenames:\n",
        "        # If there's a start and end line for that file, use it\n",
        "        try:\n",
        "            start, end = config[fname]\n",
        "        except KeyError:\n",
        "            start = None\n",
        "            end = None\n",
        "\n",
        "        # Opens the file, slices the configures lines (if any)\n",
        "        # cleans line breaks and uses the sentence tokenizer\n",
        "        with open(os.path.join(source, fname), 'r') as f:\n",
        "            contents = (''.join(f.readlines()[slice(start, end, None)])\n",
        "                        .replace('\\n', ' ').replace('\\r', ''))\n",
        "        corpus = sent_tokenize(contents, **kwargs)\n",
        "\n",
        "        # Builds a CSV file containing tokenized sentences\n",
        "        base = os.path.splitext(fname)[0]\n",
        "        new_fname = f'{base}.sent.csv'\n",
        "        new_fname = os.path.join(source, new_fname)\n",
        "        with open(new_fname, 'w') as f:\n",
        "            # Header of the file\n",
        "            if include_header:\n",
        "                if include_source:\n",
        "                    f.write('sentence,source\\n')\n",
        "                else:\n",
        "                    f.write('sentence\\n')\n",
        "            # Writes one line for each sentence\n",
        "            for sentence in corpus:\n",
        "                if include_source:\n",
        "                    f.write(f'{quote_char}{sentence}{quote_char}{sep_char}{fname}\\n')\n",
        "                else:\n",
        "                    f.write(f'{quote_char}{sentence}{quote_char}\\n')\n",
        "        new_fnames.append(new_fname)\n",
        "\n",
        "    # Returns list of the newly generated CSV files\n",
        "    return sorted(new_fnames)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function above:\n",
        "- takes a folder and goes through the file with the right extension\n",
        "- it removes the lines based on a ```.cfg``` file (if any)\n",
        "- it applies the sentence tokenizer to each file\n",
        "- it generates the corresponding CSV files of sentences using the configured ```quote_char``` and ```sep_char```\n",
        "- it names the CSV files for each original file by dropping the extension and appending ```.sent.csv``` to it."
      ],
      "metadata": {
        "id": "CUxI0S-qUVIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duMi-DzQPIov"
      },
      "outputs": [],
      "source": [
        "new_fnames = sentence_tokenize(localfolder)\n",
        "new_fnames"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each CSV file contains the sentences of a book and we will use them to build our dataset (concatenating and shuffling)"
      ],
      "metadata": {
        "id": "RxFKwhm1VF54"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNMT7-ClPIow"
      },
      "source": [
        "## HuggingFace's Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5T31ZJtPIow"
      },
      "source": [
        "## Loading a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using regular PyTorch ```Dataset``` we use **Hugging face*** ```Dataset```.\n",
        "\n",
        "In order to accomplish our task, we will use a pre-trained method coming from hugging face."
      ],
      "metadata": {
        "id": "WeARfLgKVabX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging face\n",
        "\n",
        "Hugging Face is a company and open-source platform focused on making advanced machine learning, particularly natural language processing (NLP), more accessible and easier to use. It’s widely recognized for its transformer library and model hub, which provide pre-trained models and tools to streamline the use of state-of-the-art AI models in applications.\n",
        "\n",
        "Here are the key aspects of Hugging Face:\n",
        "\n",
        "1. Transformers Library\n",
        "Hugging Face’s transformers library is one of the most popular libraries for NLP and beyond, offering access to hundreds of pre-trained models like BERT, GPT, RoBERTa, T5, and many others.\n",
        "It allows users to load and fine-tune these models for tasks like text classification, translation, summarization, question answering, text generation, and more.\n",
        "The library supports multiple deep learning frameworks, primarily PyTorch and TensorFlow, and includes APIs that make it easy to switch between them.\n",
        "\n",
        "2. Model Hub\n",
        "The Hugging Face Model Hub is a repository and sharing platform for machine learning models. It hosts thousands of pre-trained models contributed by both Hugging Face and the community.\n",
        "Users can find, share, and download models for various tasks (not limited to NLP, but also including computer vision, audio, and more).\n",
        "The hub allows seamless model access via model IDs that can be loaded directly into the transformers library, enabling rapid experimentation and deployment.\n",
        "\n",
        "3. Datasets Library\n",
        "Hugging Face’s datasets library provides a large collection of datasets for NLP, computer vision, and audio tasks. It includes popular datasets like IMDB, SQuAD, and ImageNet.\n",
        "It supports streaming, efficient data handling, and pre-processing tools, making it easier to train models on large datasets."
      ],
      "metadata": {
        "id": "U_qE0BL6WBRE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mmR-5ItPIox"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Split\n",
        "\n",
        "dataset = load_dataset(path='csv', data_files=new_fnames, quotechar='\\\\', split=Split.TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset)"
      ],
      "metadata": {
        "id": "WFzsFnVgWvTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we have done?\n",
        "\n",
        "We created a hugging face's dataset, specifying:\n",
        "1. the type of script we are using to process the data ```path='csv'``` (Yes, a bit misleading)\n",
        "2. the path where the files are located ```data_files=new_fnames```\n",
        "3. the quotation mark\n",
        "4. the split we are generating (TRAIN)"
      ],
      "metadata": {
        "id": "VEeilQsmWqt5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHikb18aPIox"
      },
      "source": [
        "### Attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvVIqWAiPIox"
      },
      "outputs": [],
      "source": [
        "dataset.features, dataset.num_columns, dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dataset contains **2** columns (sentence, source) and there are 3852 sentences on it."
      ],
      "metadata": {
        "id": "YN6gJTDYXXVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtH_c0gVPIoy"
      },
      "outputs": [],
      "source": [
        "dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDOC9Hh4PIoy"
      },
      "outputs": [],
      "source": [
        "dataset['source'][:3] ## we didnt' shuffle yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM-mEXGrPIoy"
      },
      "source": [
        "### Methods of ```Dataset```\n",
        "- unique, to compute for instance, the unique sources\n",
        "- map, to create new columns by using a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QprAteUMPIoy"
      },
      "outputs": [],
      "source": [
        "dataset.unique('source')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XbmJVItPIoz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def is_alice_label(row):\n",
        "    is_alice = int(row['source'] == 'alice28-1476.txt')\n",
        "    return {'labels': is_alice}\n",
        "\n",
        "dataset = dataset.map(is_alice_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a new column for label and it is already applied to the dataset"
      ],
      "metadata": {
        "id": "cL2hFwPxYSUT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei4rMiW0PIoz"
      },
      "outputs": [],
      "source": [
        "dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zL8_4qImPIo0"
      },
      "outputs": [],
      "source": [
        "shuffled_dataset = dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svODbO3NPIo0"
      },
      "outputs": [],
      "source": [
        "split_dataset = shuffled_dataset.train_test_split(test_size=0.2)\n",
        "split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-vhKAPsPIo0"
      },
      "outputs": [],
      "source": [
        "train_dataset, test_dataset = split_dataset['train'], split_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.features"
      ],
      "metadata": {
        "id": "uF8KbfmqYmVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9bla-vFLYtsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Apyc3vyPIo1"
      },
      "source": [
        "# Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the dataset structured with all the information we need.\n",
        "\n",
        "But the basic bricks of NLP tasks are the words, and we need to process them...\n",
        "\n",
        "We already see a simple word tokenizer that uses the white space as separator, but this doesn't work well as we want."
      ],
      "metadata": {
        "id": "JfUHYCnF9dRV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6C3Qto1PIo1"
      },
      "outputs": [],
      "source": [
        "sentence = \"I'm following the white rabbit\"\n",
        "tokens = sentence.split(' ')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use another library ```gensim``` that can help us on this task.\n",
        "\n",
        "We could use the NLKT tokenizer, but Gensim includes interesting tools that we can use later, but doesn't have a sentence tokenizer... and also it is a way to know different libraries :-)  "
      ],
      "metadata": {
        "id": "eq0d9pIL96Xk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMiBbirCPIo2"
      },
      "outputs": [],
      "source": [
        "from gensim.parsing.preprocessing import *\n",
        "\n",
        "preprocess_string(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```def preprocess_string(s, filters=DEFAULT_FILTERS)```\n",
        "Apply list of chosen filters to s.\n",
        "\n",
        "Default list of filters:\n",
        "```\n",
        "~gensim.parsing.preprocessing.strip_tags,\n",
        "~gensim.parsing.preprocessing.strip_punctuation,\n",
        "~gensim.parsing.preprocessing.strip_multiple_whitespaces,\n",
        "~gensim.parsing.preprocessing.strip_numeric,\n",
        "~gensim.parsing.preprocessing.remove_stopwords,\n",
        "~gensim.parsing.preprocessing.strip_short,\n",
        "~gensim.parsing.preprocessing.stem_text.\n",
        "```\n",
        "**Parameters**\n",
        "s : str\n",
        "filters: list of functions, optional\n",
        "\n",
        "**Returns**\n",
        "list of str\n",
        "    Processed strings (cleaned).\n",
        "\n"
      ],
      "metadata": {
        "id": "g3z27ljx_04U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We keep the list of filters limited: lower case, tags, punctuactions, multiple white spaces and numeric.\n",
        "\n",
        "You can play on your own with the filters and check the obtained results..."
      ],
      "metadata": {
        "id": "WLgc0sfJAZVI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9I0NFCcPIo2"
      },
      "outputs": [],
      "source": [
        "filters = [lambda x: x.lower(), strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric]\n",
        "preprocess_string(sentence, filters=filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another option is to use ```simple_preprocess``` method that limit the filters to:\n",
        "- lower case\n",
        "- remove tokens too short (less than 3 chars)\n",
        "- remove tokens too long (more than 15 chars)"
      ],
      "metadata": {
        "id": "R_4euJMHAwLS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6zIZlDPPIo2"
      },
      "outputs": [],
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "tokens = simple_preprocess(sentence)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S41E3OspPIo3"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the token, a list of words, we can build our vocabulary, a list of unique words that appear in the text corpora."
      ],
      "metadata": {
        "id": "bwSO0janBBTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj8VJgZ-PIo3"
      },
      "outputs": [],
      "source": [
        "sentences = train_dataset['sentence'] ## we take the column sentence\n",
        "tokens = [simple_preprocess(sent) for sent in sentences] # for each sentence we extract the tokens\n",
        "tokens[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmGTVyKkPIo3"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Once we have the tokens we can build our vocabulary, a list of unique words that appear in the text corpora.\n",
        "'''\n",
        "\n",
        "from gensim import corpora\n",
        "\n",
        "dictionary = corpora.Dictionary(tokens)\n",
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Dictionary object computes some specific attributes"
      ],
      "metadata": {
        "id": "9irmAF4pBj5c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbIfKFBIPIo3"
      },
      "outputs": [],
      "source": [
        "dictionary.num_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOPK8ULTPIo4"
      },
      "outputs": [],
      "source": [
        "dictionary.num_pos # processed words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cR7uPunPIo4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "dictionary.token2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2R6GIb0PIo5"
      },
      "outputs": [],
      "source": [
        "vocab = list(dictionary.token2id.keys())\n",
        "vocab[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaZdrWZDPIo5"
      },
      "outputs": [],
      "source": [
        "#collection frequencies, how many times a given token appears in the corpora\n",
        "dictionary.cfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f_mMaGqPIo6"
      },
      "outputs": [],
      "source": [
        "# in how many documents a token appears\n",
        "dictionary.dfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8trSPV_PIo6"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We can convert a list of tokens into a list of their corresponding indices in\n",
        "the vocabulary.\n",
        "'''\n",
        "\n",
        "\n",
        "sentence = 'follow the white rabbit'\n",
        "new_tokens = simple_preprocess(sentence)\n",
        "ids = dictionary.doc2idx(new_tokens)\n",
        "print(new_tokens)\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite the size of our vocabulary, we need to think that there will always be a word that isn't include...\n",
        "\n",
        "Therefore, we use special token:\n",
        "\n",
        "- ```[UNK]``` for unknown\n",
        "- ```[PAD]``` to pad the short sentences\n",
        "\n",
        "We can add them to our voc."
      ],
      "metadata": {
        "id": "pNPIlHgACtC3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTnw8JNXPIo6"
      },
      "outputs": [],
      "source": [
        "special_tokens = {'[PAD]': 0, '[UNK]': 1}\n",
        "dictionary.patch_with_special_tokens(special_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we could also be interested in removing rare terms as well as 'bad words'.\n",
        "\n",
        "Gensim has a couple of methods that can help on that:\n",
        "- ```filter_extremes()```\n",
        "- ```filter_tokens()```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JRb_EeBNDW6e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y0NKpAuPIo7"
      },
      "outputs": [],
      "source": [
        "def get_rare_ids(dictionary, min_freq):\n",
        "    rare_ids = [t[0] for t in dictionary.cfs.items() if t[1] < min_freq]\n",
        "    return rare_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO9cRL8fPIo7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "The code below wraps everything togheter:\n",
        "it takes a list of sentences, generates the corresponding vocabulary\n",
        "and it saves it\n",
        "'''\n",
        "\n",
        "def make_vocab(sentences, folder=None, special_tokens=None, vocab_size=None, min_freq=None):\n",
        "    if folder is not None:\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "    # tokenizes the sentences and create a Dictionary\n",
        "    tokens = [simple_preprocess(sent) for sent in sentences]\n",
        "    dictionary = corpora.Dictionary(tokens)\n",
        "    # keeps only the most frequent words (vocab size)\n",
        "    if vocab_size is not None:\n",
        "        dictionary.filter_extremes(keep_n=vocab_size)\n",
        "    # removes rare words (in case the vocab size still\n",
        "    # includes words with low frequency)\n",
        "    if min_freq is not None:\n",
        "        rare_tokens = get_rare_ids(dictionary, min_freq)\n",
        "        dictionary.filter_tokens(bad_ids=rare_tokens)\n",
        "    # gets the whole list of tokens and frequencies\n",
        "    items = dictionary.cfs.items()\n",
        "    # sorts the tokens in descending order\n",
        "    words = [dictionary[t[0]] for t in sorted(dictionary.cfs.items(), key=lambda t: -t[1])]\n",
        "    # prepends special tokens, if any\n",
        "    if special_tokens is not None:\n",
        "        to_add = []\n",
        "        for special_token in special_tokens:\n",
        "            if special_token not in words:\n",
        "                to_add.append(special_token)\n",
        "        words = to_add + words\n",
        "\n",
        "    with open(os.path.join(folder, 'vocab.txt'), 'w') as f:\n",
        "        for word in words:\n",
        "            f.write(f'{word}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x1exiX0PIo7"
      },
      "outputs": [],
      "source": [
        "make_vocab(train_dataset['sentence'], 'our_vocab/', special_tokens=['[PAD]', '[UNK]', '[SEP]', '[CLS]', '[MASK]'], min_freq=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Cq5LmV6PIo7"
      },
      "source": [
        "## HugginFace's Tokenizer\n",
        "\n",
        "Since we are going to use BERT, we will use the corresponding pre-trained tokenizer.\n",
        "\n",
        "It standardize, in some sense, the input for BERT.\n",
        "It has the same information as gensim, i.e., the mapping between tokens and their id, but it includes many more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbW7Cd8jPIo8"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "It takes a vocabulary as input\n",
        "'''\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer('our_vocab/vocab.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhxCrkJjPIo8"
      },
      "outputs": [],
      "source": [
        "new_sentence = 'follow the white rabbit neo'\n",
        "new_tokens = tokenizer.tokenize(new_sentence)\n",
        "new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6DlCKGxPIo8"
      },
      "outputs": [],
      "source": [
        "new_ids = tokenizer.convert_tokens_to_ids(new_tokens)\n",
        "new_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06gmmhDvPIo9"
      },
      "outputs": [],
      "source": [
        "new_ids = tokenizer.encode(new_sentence)\n",
        "new_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98Ccyt_LPIo9"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(new_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Transformer-based models, particularly BERT (Bidirectional Encoder Representations from Transformers), the [CLS] token stands for Classification token.\n",
        "\n",
        "**Purpose of the [CLS] Token**\n",
        "\n",
        "**Representation**: The [CLS] token is a special token added to the beginning of every input sequence in models like BERT. During training, the model learns to treat the embedding of this token as a representation of the entire sequence.\n",
        "\n",
        "**Classification Tasks**: For tasks that involve classification (e.g., sentiment analysis, sentence classification, or Next Sentence Prediction), the final hidden state of the [CLS] token (after processing through all transformer layers) is used as a summary of the entire sequence.\n",
        "\n",
        "**General-Purpose Embedding**: It’s often considered the “pooled” output, as it attempts to capture information from the whole sequence in a single vector. This embedding can then be passed to a classifier or other downstream layers for decision-making.\n"
      ],
      "metadata": {
        "id": "zOsE-oBWG1g8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVOy3y3SPIo9"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(new_sentence, add_special_tokens=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGBDu8aoPIo9"
      },
      "outputs": [],
      "source": [
        "tokenizer(new_sentence, add_special_tokens=False, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB8-t1puPIo_"
      },
      "outputs": [],
      "source": [
        "sentence1 = 'follow the white rabbit neo'\n",
        "sentence2 = 'no one can be told what the matrix is'\n",
        "joined_sentences = tokenizer(sentence1, sentence2)\n",
        "joined_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 level of information:\n",
        "- input_ids $⇒$ OK! we know them\n",
        "- token_type_ids works as sentence index\n",
        "- attention_mask that corresponds to our source mask"
      ],
      "metadata": {
        "id": "lVBzRfSRHb6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjJ5EQqfPIo_"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(tokenizer.convert_ids_to_tokens(joined_sentences['input_ids']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNHKY9CrPIo_"
      },
      "outputs": [],
      "source": [
        "separate_sentences = tokenizer([sentence1, sentence2], padding=True)\n",
        "separate_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQDFqKbPIpF"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(separate_sentences['input_ids'][0]))\n",
        "print(separate_sentences['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqsq7GS3PIpG"
      },
      "outputs": [],
      "source": [
        "first_sentences = [sentence1, 'another first sentence']\n",
        "second_sentences = [sentence2, 'a second sentence here']\n",
        "batch_of_pairs = tokenizer(first_sentences, second_sentences)\n",
        "first_input = tokenizer.convert_ids_to_tokens(batch_of_pairs['input_ids'][0])\n",
        "second_input = tokenizer.convert_ids_to_tokens(batch_of_pairs['input_ids'][1])\n",
        "print(first_input)\n",
        "print(second_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2U40RGXSPIpG"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(dataset['sentence'],\n",
        "                              padding=True,\n",
        "                              return_tensors='pt',\n",
        "                              max_length=50,\n",
        "                              truncation=True)\n",
        "tokenized_dataset['input_ids']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In reality, BERT uses vectors to represent the words, using a big look-up table with the token ids as indeces.\n",
        "\n",
        "This recalls us... **EMBEDDINGS**, in this case **WORD EMBEDDINGS**, so a representation of each token (word) as a vector, a vector of numbers... The size of the vector is the dimensionality of the embeddings.\n",
        "\n",
        "We can build the embeddings or we can learn... then we start a beautiful trip into that."
      ],
      "metadata": {
        "id": "JS9BuNz7JlLa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYIpul-cPIpG"
      },
      "source": [
        "# Before Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U80QzETEPIpH"
      },
      "source": [
        "## One-Hot Encoding (OHE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NS3mlNgIPIpH"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/ohe1.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLGxP_hdPIpH"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/ohe2.png?raw=1)\n",
        "\n",
        "As you can imagine, a representation like this has some issues:\n",
        "- large\n",
        "- sparse (lots of zeros!!!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K2WB4P5PIpH"
      },
      "source": [
        "\n",
        "## Bag of Words (BoW)\n",
        "\n",
        "Of course, we can do better. We could, for instance, sum up the corresponding OHE vectors, disregarding any underlying structure of relationships between the words.\n",
        "\n",
        "The result is the counts of the words appearing in the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6ljfjKBPIpH"
      },
      "outputs": [],
      "source": [
        "sentence = 'the white rabbit is a rabbit'\n",
        "bow_tokens = simple_preprocess(sentence)\n",
        "bow_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLrldcH-PIpI"
      },
      "outputs": [],
      "source": [
        "bow = dictionary.doc2bow(bow_tokens)\n",
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also this approach has several limitations:\n",
        "- it represents the frequencies, nothing else\n",
        "- the representations can be really different when we compute a similarity function (hortogonal in the case of OHE)\n",
        "\n",
        "Language models, instead, try to explore the structure and the relationships between words"
      ],
      "metadata": {
        "id": "QxgviixTLRi0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfbvtiv2PIpI"
      },
      "source": [
        "## Language Models\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/blank1.png?raw=1)\n",
        "\n",
        "It is easy to fill the gap with YOU\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/fill1.png?raw=1)\n",
        "\n",
        "What about this?\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/blank2.png?raw=1)\n",
        "\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/fill2.png?raw=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-grams\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/ngrams.png?raw=1)\n",
        "\n",
        "n-grams is base on pure statistics, filling the blanks using the most common sequence that matches the words precceding the blank\n",
        "\n",
        "With a large $n$, you might get good predictions, but with many cases with 0 predictions, on the contrary you may encounter many prediction errors.\n",
        "\n",
        "This is due to the fact that we only **look back**.\n",
        "\n",
        "Let's try to look ahead too!"
      ],
      "metadata": {
        "id": "h8-lIRESMT3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Bag-of-Words (CBoW)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/blank_end.png?raw=1)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/blank_center.png?raw=1)"
      ],
      "metadata": {
        "id": "6tiOfDgJMVR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It sums up (or averages) the vectors of the context words and uses it to predit the central word.\n",
        "\n",
        "The vectors are not one-hot-encoded and have continous values.\n",
        "The vector containing the continous values are called word embeddings.\n",
        "\n",
        "We can learn these vectors..\n"
      ],
      "metadata": {
        "id": "hKKdM9tJNN4j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxDgKYloPIpI"
      },
      "source": [
        "# Word Embeddings\n",
        "\n",
        "## Word2Vec\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/cbow.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **target** is the central word, therefore we deal with a multiclass classification problem, where the number of classes is the size of the vocabulary.\n",
        "\n",
        "We use the context words, better their embedding vectors, as input. Therefore becoming a parameter itself of the model.\n",
        "\n",
        "They are randomly initialized, then as the training progresses, their weights and biases are updated.\n",
        "\n",
        "**How it works**\n",
        "\n",
        "For each pair of the context words, and the corresponding target, the model will average the embeddings of the context and feed the result to a linear layer that will compute one logit for each word in the vocabulary\n"
      ],
      "metadata": {
        "id": "T0H2EupHN2IL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atT-CpiCPIpI"
      },
      "outputs": [],
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.linear = nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        embeddings = self.embedding(X)\n",
        "        bow = embeddings.mean(dim=1)\n",
        "        logits = self.linear(bow)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn-7a8LHPIpJ"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "dummy_cbow = CBOW(vocab_size=5, embedding_size=3)\n",
        "dummy_cbow.embedding.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PavMubbxPIpJ"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/w2v_embed.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Embeddings layer is a lookup table. It may be randomly initialized given the size of the vocabulary and the number of dimensions."
      ],
      "metadata": {
        "id": "AY2kJy8ZPFca"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ4ZKqOCPIpJ"
      },
      "outputs": [],
      "source": [
        "# tokens: ['is', 'barking']\n",
        "dummy_cbow.embedding(torch.as_tensor([2, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretending we have performed a tokenization and we have the indices of our vocab, context and target"
      ],
      "metadata": {
        "id": "BHgq4eP_QPb5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xeo7CI0FPIpK"
      },
      "outputs": [],
      "source": [
        "tiny_vocab = ['the', 'small', 'is', 'barking', 'dog']\n",
        "context_words = ['the', 'small', 'is', 'barking']\n",
        "target_words = ['dog']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64hplr5JPIpK"
      },
      "outputs": [],
      "source": [
        "batch_context = torch.as_tensor([[0, 1, 2, 3]]).long()\n",
        "batch_target = torch.as_tensor([4]).long()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uobj-IG8PIpK"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/w2v_cbow.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMfauJsZPIpK"
      },
      "outputs": [],
      "source": [
        "cbow_features = dummy_cbow.embedding(batch_context).mean(dim=1)\n",
        "cbow_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e19wHR2-PIpK"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/w2v_logits.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e5bcIGOPIpL"
      },
      "outputs": [],
      "source": [
        "logits = dummy_cbow.linear(cbow_features)\n",
        "logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_RwBJSPIpL"
      },
      "source": [
        "## What is an Embeddings Anyway?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a representation, and each dimension of the vector corresponds to an attribute/feature.\n",
        "\n",
        "For instance, we can describe restaurants with 3 features.\n",
        "\n",
        "We can represent the values in numbers ranging in the interval [-1,1]\n",
        "\n",
        "... and we can compute a similarity among them, cosine?"
      ],
      "metadata": {
        "id": "U9W5VgbeRWmL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u0vTklVPIpL"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/rest_discrete.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUBiuMFyPIpL"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/rest_continuous.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPzPn8PaPIpM"
      },
      "outputs": [],
      "source": [
        "ratings = torch.as_tensor([[.7, -.4, .7],\n",
        "                           [.3, .7, -.5],\n",
        "                           [.9, -.55, .8],\n",
        "                           [-.3, .8, .34]]).float()\n",
        "sims = torch.zeros(4, 4)\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        sims[i, j] = F.cosine_similarity(ratings[i], ratings[j], dim=0)\n",
        "sims"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "sns.heatmap(sims.detach().numpy(), annot=True, ax=ax)"
      ],
      "metadata": {
        "id": "ySULqbM3SJQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkxlGEraPIpM"
      },
      "source": [
        "## Pre-trained Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, pretrained-word2vec models and the embeddings correspoding to a word do not have a clear meaning, but we can do fancy stuff with them.\n",
        "\n",
        "We don't know if the $n-th$ dimension correspond to a particular \"behaviour\" of the word"
      ],
      "metadata": {
        "id": "xLgaSMplSfIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train word2vec that still is a simple model it requires a certain amount of data to train it.\n",
        "\n",
        "Luckly someone already did it the job for us and for instance gensim contains a variety of pre-trained word embeddings models.\n",
        "\n",
        "But why different models and therefore different embeddings?\n",
        "Well, using different corpora produces different embeddings, since they might be influenced by the **kind of language** used in the corpora\n",
        "\n",
        "They might depend on the model used to learn the embeddings, word2vec is one but there are many others."
      ],
      "metadata": {
        "id": "UKzRLxW7S-_u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vrvd1NarPIpM"
      },
      "source": [
        "## Global Vectors (GloVe)\n",
        "\n",
        "GloVe: Global Vectors for Word Representation\n",
        "\n",
        "It combines skip-gram model with co-occurences statistics at the **global level**\n",
        "\n",
        "Take it for grant, if you want to know more, you can read the paper and check [https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many sizes and shapes, dimensions from 25 to 300 and vocab size between 400,000 and 2,200,000 words."
      ],
      "metadata": {
        "id": "QC3dftjRUk0F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl_JFk35PIpM"
      },
      "outputs": [],
      "source": [
        "from gensim import downloader\n",
        "\n",
        "glove = downloader.load('glove-wiki-gigaword-50')\n",
        "\n",
        "\n",
        "len(glove.key_to_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDzgsj0bPIpN"
      },
      "outputs": [],
      "source": [
        "glove['alice']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't know the meaning of the dimensions, but we can do math with them\n",
        "\n",
        "We can define the queen with the following equation:\n",
        "king - man + woman = queen"
      ],
      "metadata": {
        "id": "M1__tkUyVDrR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46hbjBCmPIpO"
      },
      "outputs": [],
      "source": [
        "synthetic_queen = glove['king'] - glove['man'] + glove['woman']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcrptYwOPIpP"
      },
      "outputs": [],
      "source": [
        "fig = plot_word_vectors(glove,\n",
        "                        ['king', 'man', 'woman', 'synthetic', 'queen'],\n",
        "                        other={'synthetic': synthetic_queen})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtLfGWX7PIpP"
      },
      "outputs": [],
      "source": [
        "glove.similar_by_vector(synthetic_queen, topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's pretty common that the first results corresponds to the origin of the word embeddings arithmetic, so we can exclude it from the result... and we get queen :-)"
      ],
      "metadata": {
        "id": "Tao7eIHEVuhX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEk297ScPIpQ"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/embed_arithmetic.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uv9IVvGPIpQ"
      },
      "source": [
        "$$\n",
        "\\Large\n",
        "w_{\\text{king}} - w_{\\text{man}}\\approx w_{\\text{queen}}-w_{\\text{woman}} \\implies w_{\\text{king}} - w_{\\text{man}} + w_{\\text{woman}} \\approx w_{\\text{queen}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is nice and it shows that effectively the embeddings are capturing the meaning/semantics of the words."
      ],
      "metadata": {
        "id": "h87MLW3eWED7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7M0z1YMPIpQ"
      },
      "source": [
        "## Using Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuAY6Gk6PIpQ"
      },
      "source": [
        "### Vocabulary Coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpLS4nhMPIpQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "vocab = list(dictionary.token2id.keys())\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWvfyewTPIpQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "unknown_words = sorted(list(set(vocab).difference(set(glove.key_to_index))))\n",
        "###########################################################\n",
        "print(len(unknown_words))\n",
        "print(unknown_words[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EddC0V1zPIpR"
      },
      "outputs": [],
      "source": [
        "unknown_ids = [dictionary.token2id[w] for w in unknown_words if w not in ['[PAD]', '[UNK]']]\n",
        "unknown_count = np.sum([dictionary.cfs[idx] for idx in unknown_ids])\n",
        "unknown_count, dictionary.num_pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWUf3u6CPIpR"
      },
      "outputs": [],
      "source": [
        "def vocab_coverage(gensim_dict, pretrained_wv, special_tokens=('[PAD]', '[UNK]')):\n",
        "    vocab = list(gensim_dict.token2id.keys())\n",
        "    unknown_words = sorted(list(set(vocab).difference(set(pretrained_wv.key_to_index))))\n",
        "    ###########################################################\n",
        "    unknown_ids = [gensim_dict.token2id[w] for w in unknown_words if w not in special_tokens]\n",
        "    unknown_count = np.sum([gensim_dict.cfs[idx] for idx in unknown_ids])\n",
        "    cov = 1 - unknown_count / gensim_dict.num_pos\n",
        "    return cov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYZTCStNPIpR"
      },
      "outputs": [],
      "source": [
        "vocab_coverage(dictionary, glove)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKzOosVxPIpR"
      },
      "source": [
        "### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf8fyy3mPIpS"
      },
      "outputs": [],
      "source": [
        "def make_vocab_from_wv(wv, folder=None, special_tokens=None):\n",
        "    if folder is not None:\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "    words = wv.index_to_key\n",
        "    ###########################################################\n",
        "    if special_tokens is not None:\n",
        "        to_add = []\n",
        "        for special_token in special_tokens:\n",
        "            if special_token not in words:\n",
        "                to_add.append(special_token)\n",
        "        words = to_add + words\n",
        "\n",
        "    with open(os.path.join(folder, 'vocab.txt'), 'w') as f:\n",
        "        for word in words:\n",
        "            f.write(f'{word}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4Y-ub7iPIpS"
      },
      "outputs": [],
      "source": [
        "make_vocab_from_wv(glove, 'glove_vocab/', special_tokens=['[PAD]', '[UNK]'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NIjvZpFPIpS"
      },
      "outputs": [],
      "source": [
        "glove_tokenizer = BertTokenizer('glove_vocab/vocab.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN_vm748PIpS"
      },
      "outputs": [],
      "source": [
        "glove_tokenizer.encode('alice followed the white rabbit', add_special_tokens=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwsVqJhtPIpS"
      },
      "outputs": [],
      "source": [
        "len(glove_tokenizer.vocab), len(glove.vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The difference is given by the two special tokens [PAD] and [UNK] and we can add them to our embeddings with all zeros"
      ],
      "metadata": {
        "id": "AR_dv0CcXM93"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBrqOhP-PIpT"
      },
      "source": [
        "### Special Tokens' Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzAqTQ9CPIpT"
      },
      "outputs": [],
      "source": [
        "special_embeddings = np.zeros((2, glove.vector_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AWIyA3JPIpT"
      },
      "outputs": [],
      "source": [
        "extended_embeddings = np.concatenate([special_embeddings, glove.vectors], axis=0)\n",
        "extended_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMTUfdESPIpT"
      },
      "outputs": [],
      "source": [
        "alice_idx = glove_tokenizer.encode('alice', add_special_tokens=False)\n",
        "np.all(extended_embeddings[alice_idx] == glove['alice'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8_PGFaXPIpU"
      },
      "source": [
        "## Model I - GloVe + Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzpeWYcdPIpU"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOc1ZhgdPIpU"
      },
      "outputs": [],
      "source": [
        "train_sentences = train_dataset['sentence']\n",
        "train_labels = train_dataset['labels']\n",
        "\n",
        "test_sentences = test_dataset['sentence']\n",
        "test_labels = test_dataset['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5SLBMioPIpU"
      },
      "outputs": [],
      "source": [
        "train_ids = glove_tokenizer(train_sentences,\n",
        "                            truncation=True,\n",
        "                            padding=True,\n",
        "                            max_length=60,\n",
        "                            add_special_tokens=False,\n",
        "                            return_tensors='pt')['input_ids']\n",
        "train_labels = torch.as_tensor(train_labels).float().view(-1, 1)\n",
        "\n",
        "test_ids = glove_tokenizer(test_sentences,\n",
        "                           truncation=True,\n",
        "                           padding=True,\n",
        "                           max_length=60,\n",
        "                           add_special_tokens=False,\n",
        "                           return_tensors='pt')['input_ids']\n",
        "test_labels = torch.as_tensor(test_labels).float().view(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Gy9RIDEPIpU"
      },
      "outputs": [],
      "source": [
        "train_tensor_dataset = TensorDataset(train_ids, train_labels)\n",
        "generator = torch.Generator()\n",
        "train_loader = DataLoader(train_tensor_dataset, batch_size=32, shuffle=True, generator=generator)\n",
        "test_tensor_dataset = TensorDataset(test_ids, test_labels)\n",
        "test_loader = DataLoader(test_tensor_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-3CdOpWPIpU"
      },
      "source": [
        "### Pre-Trained PyTorch Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGxrKlyDPIpV"
      },
      "outputs": [],
      "source": [
        "extended_embeddings = torch.as_tensor(extended_embeddings).float()\n",
        "torch_embeddings = nn.Embedding.from_pretrained(extended_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpdJmQNsPIpV"
      },
      "outputs": [],
      "source": [
        "token_ids, labels = next(iter(train_loader))\n",
        "token_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwyGjFNSPIpV"
      },
      "outputs": [],
      "source": [
        "token_embeddings = torch_embeddings(token_ids)\n",
        "token_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the ids to get the embeddings. since we have 32 sentences, of 60 tokens with 50 dimensions each\n"
      ],
      "metadata": {
        "id": "WzDVJ84aYKcV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIHGuQ4WPIpV"
      },
      "outputs": [],
      "source": [
        "token_embeddings.mean(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each sentence, we can compute an embedding as an average of the word embeddings and therefore we can use it as features for a classification algorithm"
      ],
      "metadata": {
        "id": "5c5IbeKtYekv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PT2eZ1EqPIpV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "we can use the PyTorch implementation that is nnEmbeddingBag\n",
        "'''\n",
        "\n",
        "boe_mean = nn.EmbeddingBag.from_pretrained(extended_embeddings, mode='mean')\n",
        "boe_mean(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDyYigyhPIpV"
      },
      "source": [
        "### Model Configuration & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yaB00G4PIpW"
      },
      "outputs": [],
      "source": [
        "extended_embeddings = torch.as_tensor(extended_embeddings).float()\n",
        "boe_mean = nn.EmbeddingBag.from_pretrained(\n",
        "    extended_embeddings, mode='mean'\n",
        ")\n",
        "torch.manual_seed(41)\n",
        "model = nn.Sequential(\n",
        "    # Embeddings\n",
        "    boe_mean,\n",
        "    # Classifier\n",
        "    nn.Linear(boe_mean.embedding_dim, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 1)\n",
        ")\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTWpbPdfPIpW"
      },
      "outputs": [],
      "source": [
        "sbs_emb = StepByStep(model, loss_fn, optimizer)\n",
        "sbs_emb.set_loaders(train_loader, test_loader)\n",
        "sbs_emb.train(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiaVP9uvPIpW"
      },
      "outputs": [],
      "source": [
        "fig = sbs_emb.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCKFSECRPIpW"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(test_loader, sbs_emb.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kqa22QNPIpX"
      },
      "source": [
        "## Model II - GloVe + Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "An instance of a transformer encoder, a layer of pre-trained embeddings and the desired number of outputs.\n",
        "\n",
        "forward takes minibatches of tokenized sentences, preprocess them, encodes them and output the logits."
      ],
      "metadata": {
        "id": "48ylfKoXaCXx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH3DdVEFPIpX"
      },
      "outputs": [],
      "source": [
        "class TransfClassifier(nn.Module):\n",
        "    def __init__(self, embedding_layer, encoder, n_outputs):\n",
        "        super().__init__()\n",
        "        self.d_model = encoder.d_model\n",
        "        self.n_outputs = n_outputs\n",
        "        self.encoder = encoder\n",
        "        self.mlp = nn.Linear(self.d_model, n_outputs)\n",
        "\n",
        "        self.embed = embedding_layer\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.d_model))\n",
        "\n",
        "    def preprocess(self, X):\n",
        "        # N, L -> N, L, D\n",
        "        src = self.embed(X)\n",
        "        # Special classifier token\n",
        "        # 1, 1, D -> N, 1, D\n",
        "        cls_tokens = self.cls_token.expand(X.size(0), -1, -1)\n",
        "        # Concatenates CLS tokens -> N, 1 + L, D\n",
        "        src = torch.cat((cls_tokens, src), dim=1)\n",
        "        return src\n",
        "\n",
        "    def encode(self, source, source_mask=None):\n",
        "        # Encoder generates \"hidden states\"\n",
        "        states = self.encoder(source, source_mask)\n",
        "        # Gets state from first token only: [CLS]\n",
        "        cls_state = states[:, 0]  # N, 1, D\n",
        "        return cls_state\n",
        "\n",
        "    @staticmethod\n",
        "    def source_mask(X):\n",
        "        cls_mask = torch.ones(X.size(0), 1).type_as(X)\n",
        "        pad_mask = torch.cat((cls_mask, X > 0), dim=1).bool()\n",
        "        return pad_mask.unsqueeze(1)\n",
        "\n",
        "    def forward(self, X):\n",
        "        src = self.preprocess(X)\n",
        "        # Featurizer\n",
        "        cls_state = self.encode(src, self.source_mask(X))\n",
        "        # Classifier\n",
        "        out = self.mlp(cls_state) # N, 1, outputs\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUum393hPIpX"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(33)\n",
        "# Loads the pretrained GloVe embeddings into an embedding layer\n",
        "torch_embeddings = nn.Embedding.from_pretrained(extended_embeddings)\n",
        "# Creates a Transformer Encoder\n",
        "layer = EncoderLayer(n_heads=2, d_model=torch_embeddings.embedding_dim, ff_units=128)\n",
        "encoder = EncoderTransf(layer, n_layers=1)\n",
        "# Uses both layers above to build our model\n",
        "model = TransfClassifier(torch_embeddings, encoder, n_outputs=1)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mBjsLwHPIpX"
      },
      "outputs": [],
      "source": [
        "sbs_transf = StepByStep(model, loss_fn, optimizer)\n",
        "sbs_transf.set_loaders(train_loader, test_loader)\n",
        "sbs_transf.train(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CwpEol1PIpY"
      },
      "outputs": [],
      "source": [
        "fig = sbs_transf.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPQvdYWVPIpY"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(test_loader, sbs_transf.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xh8xxgTPIpY"
      },
      "source": [
        "### Visualizing Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5_67okaPIpY"
      },
      "outputs": [],
      "source": [
        "sentences = ['The white rabbit and Alice ran away', 'The lion met Dorothy on the road']\n",
        "inputs = glove_tokenizer(sentences, add_special_tokens=False, return_tensors='pt')['input_ids']\n",
        "inputs = inputs.to(sbs_transf.device)\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cu3MXRxgPIpZ"
      },
      "outputs": [],
      "source": [
        "sbs_transf.model.eval()\n",
        "out = sbs_transf.model(inputs)\n",
        "# our model outputs logits, so we turn them into probs\n",
        "torch.sigmoid(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nld9xIhPIpZ"
      },
      "outputs": [],
      "source": [
        "alphas = sbs_transf.model.encoder.layers[0].self_attn_heads.alphas\n",
        "alphas[:, :, 0, :].squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5gATXCkPIpZ"
      },
      "outputs": [],
      "source": [
        "tokens = [['[CLS]'] + glove_tokenizer.tokenize(sent) for sent in sentences]\n",
        "fig = plot_attention(tokens, alphas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNXPXiSlPIpa"
      },
      "source": [
        "# Contextual Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5oqS0x7PIpa"
      },
      "source": [
        "## ELMo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watch in these two sentences has a different meaning noun and verb. Probabily, the single word embedding is not enough... we need to consider the context, the sentence itself to represent the word.\n",
        "\n",
        "These are called contextual word embeddings where we don't have a look-up table between every combination of word and context, but the embeddings are the outputs of a model :-)"
      ],
      "metadata": {
        "id": "hHA35sJ1buEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ELMo takes into account also the context.\n",
        "\n",
        "It is a two layer bidirectional LSTM encoder using 4096 dimensions for its cell states\n",
        "\n",
        "The representations are char-based, so it can easily handle unkown words\n",
        "\n",
        "Flair is a NLP (yet another) library built on top of pytorch that offers word embeddings and document embeddings for ELMo and BERT as well as GloVe."
      ],
      "metadata": {
        "id": "H1Ksw21GcbMW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N8dJJJmPIpa"
      },
      "outputs": [],
      "source": [
        "watch1 = \"\"\"\n",
        "The Hatter was the first to break the silence. `What day of the month is it?' he said, turning to Alice:  he had taken his watch out of his pocket, and was looking at it uneasily, shaking it every now and then, and holding it to his ear.\n",
        "\"\"\"\n",
        "\n",
        "watch2 = \"\"\"\n",
        "Alice thought this a very curious thing, and she went nearer to watch them, and just as she came up to them she heard one of them say, `Look out now, Five!  Don't go splashing paint over me like that!\n",
        "\"\"\"\n",
        "\n",
        "sentences = [watch1, watch2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M_qVWdNPIpa"
      },
      "outputs": [],
      "source": [
        "from flair.data import Sentence\n",
        "\n",
        "flair_sentences = [Sentence(s) for s in sentences]\n",
        "flair_sentences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsasoxvnPIpb"
      },
      "outputs": [],
      "source": [
        "flair_sentences[0].get_token(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdaPu5VNPIpb"
      },
      "outputs": [],
      "source": [
        "flair_sentences[0].tokens[31]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_iTjhAuPIpb"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import FlairEmbeddings\n",
        "flair_emb = FlairEmbeddings('news-forward')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZOGM9xpPIpb"
      },
      "outputs": [],
      "source": [
        "flair_emb.embed(flair_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdFVdgRZPIpc"
      },
      "outputs": [],
      "source": [
        "token_watch1 = flair_sentences[0].tokens[31]\n",
        "token_watch2 = flair_sentences[1].tokens[13]\n",
        "token_watch1, token_watch2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XbrpwUfPIpc"
      },
      "outputs": [],
      "source": [
        "token_watch1.embedding, token_watch2.embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hxX2iMHPIpd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "similarity(token_watch1.embedding, token_watch2.embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF_w6p-rPIpe"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(embeddings, sentence):\n",
        "    sent = Sentence(sentence)\n",
        "    embeddings.embed(sent)\n",
        "    return torch.stack([token.embedding for token in sent.tokens]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEU5nynEPIpe",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "get_embeddings(flair_emb, watch1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX_xJposPIpe"
      },
      "source": [
        "## GloVe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7Y54X9_PIpe"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import WordEmbeddings\n",
        "glove_embedding = WordEmbeddings('glove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mibBx6mvPIpe"
      },
      "outputs": [],
      "source": [
        "new_flair_sentences = [Sentence(s) for s in sentences]\n",
        "glove_embedding.embed(new_flair_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzaqGrPBPIpf"
      },
      "outputs": [],
      "source": [
        "torch.all(new_flair_sentences[0].tokens[31].embedding == new_flair_sentences[1].tokens[13].embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIH-ZF35PIpf"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wppmv3qLPIpf"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "bert_flair = TransformerWordEmbeddings('bert-base-uncased', layers='-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXKlNCfdPIpf"
      },
      "outputs": [],
      "source": [
        "embed1 = get_embeddings(bert_flair, watch1)\n",
        "embed2 = get_embeddings(bert_flair, watch2)\n",
        "embed2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNZPqZPUPIpg"
      },
      "outputs": [],
      "source": [
        "bert_watch1 = embed1[31]\n",
        "bert_watch2 = embed2[13]\n",
        "bert_watch1, bert_watch2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvTJj1vcPIpg"
      },
      "outputs": [],
      "source": [
        "similarity = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "similarity(bert_watch1, bert_watch2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt9Wl8aIPIpg"
      },
      "source": [
        "## Document Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgU-haTVPIpg"
      },
      "outputs": [],
      "source": [
        "documents = [Sentence(watch1), Sentence(watch2)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLbRmOR_PIpg"
      },
      "outputs": [],
      "source": [
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "bert_doc = TransformerDocumentEmbeddings('bert-base-uncased')\n",
        "bert_doc.embed(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB6GBlxYPIpg"
      },
      "outputs": [],
      "source": [
        "documents[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX4ZkKaPPIph"
      },
      "outputs": [],
      "source": [
        "documents[0].tokens[31].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8XT_0ahPIph"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(embeddings, sentence):\n",
        "    sent = Sentence(sentence)\n",
        "    embeddings.embed(sent)\n",
        "    if len(sent.embedding):\n",
        "        return sent.embedding.float()\n",
        "    else:\n",
        "        return torch.stack([token.embedding for token in sent.tokens]).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yt6esWrPIph"
      },
      "outputs": [],
      "source": [
        "get_embeddings(bert_doc, watch1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp3LpDtwPIph"
      },
      "source": [
        "## Model III - Preprocessing Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to use the get_embeddings for every sentence, therefore we can use the map function of the HF dataset, then we need the embeddings to be PyTorch Tensors"
      ],
      "metadata": {
        "id": "OA8AXejefYzu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xafyHRinPIpi"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62H2IcGYPIpi"
      },
      "outputs": [],
      "source": [
        "train_dataset_doc = train_dataset.map(lambda row: {'embeddings': get_embeddings(bert_doc, row['sentence'])})\n",
        "test_dataset_doc = test_dataset.map(lambda row: {'embeddings': get_embeddings(bert_doc, row['sentence'])})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5rURvw6PIpi"
      },
      "outputs": [],
      "source": [
        "train_dataset_doc.set_format(type='torch', columns=['embeddings', 'labels'])\n",
        "test_dataset_doc.set_format(type='torch', columns=['embeddings', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Ov7ucKPIpi"
      },
      "outputs": [],
      "source": [
        "train_dataset_doc['embeddings']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfMQkFCHPIpi"
      },
      "outputs": [],
      "source": [
        "train_dataset_doc = TensorDataset(train_dataset_doc['embeddings'].float(),\n",
        "                                  train_dataset_doc['labels'].view(-1, 1).float())\n",
        "generator = torch.Generator()\n",
        "train_loader = DataLoader(train_dataset_doc, batch_size=32, shuffle=True, generator=generator)\n",
        "\n",
        "test_dataset_doc = TensorDataset(test_dataset_doc['embeddings'].float(),\n",
        "                                 test_dataset_doc['labels'].view(-1, 1).float())\n",
        "test_loader = DataLoader(test_dataset_doc, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDOr-sgLPIpi"
      },
      "source": [
        "### Model Configuration & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEjwk7i8PIpj"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(41)\n",
        "model = nn.Sequential(\n",
        "    # Classifier\n",
        "    nn.Linear(bert_doc.embedding_length, 3),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(3, 1)\n",
        ")\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93pg9nrOPIpj"
      },
      "outputs": [],
      "source": [
        "sbs_doc_emb = StepByStep(model, loss_fn, optimizer)\n",
        "sbs_doc_emb.set_loaders(train_loader, test_loader)\n",
        "sbs_doc_emb.train(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jSTsMaSPIpj"
      },
      "outputs": [],
      "source": [
        "fig = sbs_doc_emb.plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZpLkF80PIpj"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(test_loader, sbs_doc_emb.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLKZdrC9PIpj"
      },
      "source": [
        "# BERT\n",
        "\n",
        "**B**idirectional **E**ncoder **R**epresentation from **T**ransformers $→$ **BERT**\n",
        "\n",
        "It is a model based on a **transformer encoder**.\n",
        "\n",
        "It was introduced in a paper titled: *BERT: Pre-training of Bidirectional Transformers for Language Understanding* (2019)\n",
        "\n",
        "Some number to give you an idea:\n",
        "Trained on huge corpora: BookCorpus, 800M of words, 11.038 unpublished books and English Wikipedia with 2.5B of words\n",
        "\n",
        "12 layers, 12 attention heads, 768 hidden dimensions, with a total of 110 Milion Parameters.\n",
        "\n",
        "What does this mean? That we don't have -- as personal users -- the computational resources to train such a kind of models.\n",
        "\n",
        "HuggingFace is at our disposal and there are many different version of BERT available.\n",
        "\n",
        "What do we want to do now?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "USE A PRE-TRAINED VERSION OF BERT, FINE-TUNING IT FOR OUR PURPOSES AND EVALUATE ON OUR SENTENCE CLASSIFIER"
      ],
      "metadata": {
        "id": "R-iQfswqbge_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtgTF0CIPIpk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "if you want to try different models without having to import their\n",
        "corresponding classes, you can use HuggingFace's AutoModel\n",
        "\n",
        "It infers the corret model class based on the name of the model you are loading\n",
        "'''\n",
        "from transformers import AutoModel\n",
        "auto_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "print(auto_model.__class__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYjuuJG-PIpk"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Or you can import the class model\n",
        "'''\n",
        "\n",
        "from transformers import BertModel\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpDTWGNBPIpk"
      },
      "outputs": [],
      "source": [
        "bert_model.config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are able to recognize some of these parameters, right?\n",
        "hidden_size, num_attention_heads, num_hidden_layers...\n",
        "\n",
        "Some of them will be explained in few minutes.\n",
        "\n",
        "But first of all, our model needs to receive inputs and these inputs need to be **TOKENIZED**\n"
      ],
      "metadata": {
        "id": "rlxCApc4cuq4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBkvBjp5PIpk"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We can consider the tokenization as a pre-processing step, and since we are going to use a pre-trained BERT model, we need to use the same tokenizer that was used during the pre-training.\n",
        "\n",
        "So, in HF each pre-trained model has its own pre-trained tokenizer as well.\n",
        "\n",
        "Let's create our BERT tokenizer..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7ZcsRmdPIpk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "len(bert_tokenizer.vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only ```30522`` tokens...\n",
        "But in reality these are not exactly words but they may also be **word pieces**.\n",
        "\n",
        "\n",
        "Before, for words not belonging to our vocabulary we used the special token ```[UNK]```.\n",
        "This approach gets some information loss, all the unknown words are replaced with the same token.\n",
        "\n",
        "The approach defined here is a litlle bit different.\n",
        "We disassemble an unknown word into its components, and for instance the word ```inexplicably``` can be disassembled into five word pieces:\n",
        "\n",
        "```inexplicably``` $→$ ```in + ##ex + ##pl + ##ica + ##bly```  \n",
        "\n",
        "Every word pieces is prefixed with ```##``` to indicate that is doesn't stand on its own as a word.\n",
        "\n",
        "Therefore, an unknown word becomes a concatenation of **word-pieces**"
      ],
      "metadata": {
        "id": "qgpUmcFsdyUo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyF47gBWPIpk"
      },
      "outputs": [],
      "source": [
        "sentence1 = 'Alice is inexplicably following the white rabbit'\n",
        "sentence2 = 'Follow the white rabbit, Neo'\n",
        "tokens = bert_tokenizer(sentence1, sentence2, return_tensors='pt')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- input_ids contains the token id,\n",
        "- token_type_ids contains the sentence index\n",
        "- the attention mask is self explanatory."
      ],
      "metadata": {
        "id": "yVywbmABfsnJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brcF_Ox1PIpl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "We take the ids (input_ids)\n",
        "and we convert them to the corresponding word pieces (tokens)\n",
        "'''\n",
        "\n",
        "print(bert_tokenizer.convert_ids_to_tokens(tokens['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [CLS] at the start, the classifier token\n",
        "- [SEP] between the two sentences and at the end\n",
        "- inexplicably got disassemled into word pieces"
      ],
      "metadata": {
        "id": "CUimfhyQgol7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cjrk1qCPIpl"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "As for the model you can use the AutoTokenizer\n",
        "to try different tokenizers without importing their classes\n",
        "'''\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "auto_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(auto_tokenizer.__class__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0A5jpZJPIpl"
      },
      "source": [
        "## Input Embeddings\n",
        "\n",
        "Once the sentences are tokenized, we can use their tokens' IDs to look up the corresponding embeddings, as usual.\n",
        "\n",
        "1. BERT is a transformer encoder, and it needs positional information, and BERT uses **position embeddings**\n",
        "  a. position encoding used before had fixed values for each position, the **position embeddings** are learned by the model, as any other embedding layer. The number of entries is defined by the maximum length of the sequence (see parameters above).\n",
        "\n",
        "2. BERT adds a third embedding: segment embedding, which is a position embedding at the sentence level\n",
        "\n",
        "**Original Design of BERT**\n",
        "\n",
        "- BERT was designed to handle tasks involving one sentence or two sentences.\n",
        "- For single-sentence tasks (e.g., sentiment analysis), the input is just one sentence.\n",
        "- For tasks requiring two sentences (e.g., next sentence prediction or sentence-pair classification), the input consists of two segments: Sentence A [SEP] Sentence B.\n",
        "\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/bert_input_embed.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOhc49UmPIpl"
      },
      "outputs": [],
      "source": [
        "input_embeddings = bert_model.embeddings\n",
        "input_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo5g1KN2PIpm"
      },
      "outputs": [],
      "source": [
        "token_embeddings = input_embeddings.word_embeddings\n",
        "token_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30522 entries, and 768 hidden dimensions"
      ],
      "metadata": {
        "id": "U47NF6uZjXuC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr_sA4KEPIpm"
      },
      "outputs": [],
      "source": [
        "input_token_emb = token_embeddings(tokens['input_ids'])\n",
        "input_token_emb,input_token_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx5LiAACPIpm"
      },
      "outputs": [],
      "source": [
        "position_embeddings = input_embeddings.position_embeddings\n",
        "position_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6e0kmEnPIpm"
      },
      "outputs": [],
      "source": [
        "position_ids = torch.arange(512).expand((1, -1))\n",
        "position_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqZUNJBhPIpn"
      },
      "outputs": [],
      "source": [
        "seq_length = tokens['input_ids'].size(1)\n",
        "input_pos_emb = position_embeddings(position_ids[:, :seq_length])\n",
        "input_pos_emb,input_pos_emb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DXX4yiEPIpn"
      },
      "outputs": [],
      "source": [
        "segment_embeddings = input_embeddings.token_type_embeddings\n",
        "segment_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgCRoV2sPIpn"
      },
      "outputs": [],
      "source": [
        "input_seg_emb = segment_embeddings(tokens['token_type_ids'])\n",
        "input_seg_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT adds all three embeddings, then layer normalize and dropout, but these are the inputs that BERT uses"
      ],
      "metadata": {
        "id": "8c2pqJMbkans"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUkjZrKTPIpn"
      },
      "outputs": [],
      "source": [
        "input_emb = input_token_emb + input_pos_emb + input_seg_emb\n",
        "input_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGkuZ7UdPIpn"
      },
      "source": [
        "## Pretraining Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is a autoencoding model because it is a trasfomer encoder and because it was trained to reconstruct sentences from corrupted inputs.\n",
        "\n",
        "This type of Language models are called masked language models (MLM) pre-training task.\n",
        "\n",
        "It tries to predict a masked word/token that is inside a sentence, filling the blanks as the continous bag-of-words (CBoW) does.\n",
        "\n",
        "There are strategies to select which token has to be masked, the target of our encoder is the original sentence.\n",
        "\n",
        "In particular, BERT computes the logits only for the randomly masked inputs, the others are not used to compute the loss."
      ],
      "metadata": {
        "id": "rCwvtBl1lY4h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2FoG2AGPIpn"
      },
      "source": [
        "### Masked Language Model (MLM)\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/bert_mlm.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTw0eDKePIpo"
      },
      "outputs": [],
      "source": [
        "sentence = 'Alice is inexplicably following the white rabbit'\n",
        "tokens = bert_tokenizer(sentence)\n",
        "tokens['input_ids']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yCbwZXgPIpo"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "torch.manual_seed(41)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=bert_tokenizer, mlm_probability=0.15)\n",
        "mlm_tokens = data_collator([tokens])\n",
        "mlm_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI180xPwPIpo",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "print(bert_tokenizer.convert_ids_to_tokens(tokens['input_ids']))\n",
        "print(bert_tokenizer.convert_ids_to_tokens(mlm_tokens['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyTICtkbPIpo"
      },
      "source": [
        "### Next Sentence Prediction (NSP)\n",
        "\n",
        "Another pre-training task is the Next Sentence Prediction (NSP) task.\n",
        "BERT was trained to predict if a second sentence is actually the next sentence in the original text or not.\n",
        "\n",
        "In this way, the model learns the relationships between the sencences.\n",
        "\n",
        "This task takes the special classifier token [CLS] (its final hidden states) as features for a classifier.\n",
        "\n",
        "\n",
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/bert_nsp.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWrQN9DpPIpo"
      },
      "outputs": [],
      "source": [
        "bert_model.pooler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmVOtEwePIpp"
      },
      "outputs": [],
      "source": [
        "sentence1 = 'alice follows the white rabbit'\n",
        "sentence2 = 'follow the white rabbit neo'\n",
        "bert_tokenizer(sentence1, sentence2, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HkUpyLUPIpp"
      },
      "source": [
        "## Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C-AaHkFPIpp"
      },
      "outputs": [],
      "source": [
        "sentence = 'And, so far as they knew, they were quite right' #train_dataset[100]['sentence']\n",
        "sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmqPWjJuPIpp"
      },
      "outputs": [],
      "source": [
        "tokens = bert_tokenizer(sentence,\n",
        "                        padding='max_length',\n",
        "                        max_length=30,\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\")\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ssmrA7PIpp"
      },
      "outputs": [],
      "source": [
        "bert_model.eval()\n",
        "out = bert_model(input_ids=tokens['input_ids'],\n",
        "                 attention_mask=tokens['attention_mask'],\n",
        "                 output_attentions=True,\n",
        "                 output_hidden_states=True,\n",
        "                 return_dict=True)\n",
        "\n",
        "print()\n",
        "out.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-```last_hidden_state``` is returned by default and is the most importan output of the all: it contains the final hidden states for each and every token in the input, this can be seen as **contextual word embeddings**\n",
        "\n",
        "  - [CLS], [SEP], and [PAD] are also included"
      ],
      "metadata": {
        "id": "BpDgJTnioW11"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0lNYShtPIpq"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/bert_embeddings.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9qGa0qaPIpq"
      },
      "outputs": [],
      "source": [
        "last_hidden_batch = out['last_hidden_state']\n",
        "last_hidden_sentence = last_hidden_batch[0]\n",
        "# Removes hidden states for [PAD] tokens using the mask\n",
        "mask = tokens['attention_mask'].squeeze().bool()\n",
        "embeddings = last_hidden_sentence[mask]\n",
        "# Removes embeddings for the first [CLS] and last [SEP] tokens\n",
        "embeddings[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlqC7lDSPIpq"
      },
      "outputs": [],
      "source": [
        "get_embeddings(bert_flair, sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ```hidden_states``` returns hidden states for every layer in BERT encoder architecture, including the last one, and the input embedding as well."
      ],
      "metadata": {
        "id": "oK4IbZXEpmHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore 12 +1 (the input embeddings)"
      ],
      "metadata": {
        "id": "KgIVn6jAp4Uv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNKAkRLyPIpq"
      },
      "outputs": [],
      "source": [
        "print(len(out['hidden_states']))\n",
        "print(out['hidden_states'][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451QgYohPIpq"
      },
      "outputs": [],
      "source": [
        "(out['hidden_states'][0] == bert_model.embeddings(tokens['input_ids'])).all()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93o1_w-EPIpr"
      },
      "outputs": [],
      "source": [
        "(out['hidden_states'][-1] == out['last_hidden_state']).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ```pooler_output``` is returned by default, it's the output of the pooler given the last hidden state as its input"
      ],
      "metadata": {
        "id": "bGIMFpULqEaN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYA0p0XqPIpr"
      },
      "outputs": [],
      "source": [
        "(out['pooler_output'] == bert_model.pooler(out['last_hidden_state'])).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ```attentions``` return the self-attention scores for each attention head in each layer of BERT's encoder:"
      ],
      "metadata": {
        "id": "fLc74q3zqVHx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsdP4CnSPIpr"
      },
      "outputs": [],
      "source": [
        "print(len(out['attentions']))\n",
        "print(out['attentions'][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(out['attentions']))"
      ],
      "metadata": {
        "id": "z00R_ySjrMDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 elements, one for each layer, each element has a tensor containing the scores for the sentences in the mini-batch (only one in our case). Those scores include each 12 self-attention heads, each head indicating how mcuh attention each of the 30 tokens is paying to all 30 tokens."
      ],
      "metadata": {
        "id": "xdVBLOVVqiTI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlX9GImRPIpr"
      },
      "source": [
        "## Model IV - Classifying using BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWB0IsJEPIpr"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, ff_units, n_outputs, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.d_model = bert_model.config.dim\n",
        "        self.n_outputs = n_outputs\n",
        "        self.encoder = bert_model\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(self.d_model, ff_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_units, n_outputs)\n",
        "        )\n",
        "\n",
        "    def encode(self, source, source_mask=None):\n",
        "        states = self.encoder(input_ids=source,\n",
        "                              attention_mask=source_mask)[0]\n",
        "        cls_state = states[:, 0]\n",
        "        return cls_state\n",
        "\n",
        "    def forward(self, X):\n",
        "        source_mask = (X > 0)\n",
        "        # Featurizer\n",
        "        cls_state = self.encode(X, source_mask)\n",
        "        # Classifier\n",
        "        out = self.mlp(cls_state)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "our model takes\n",
        "- an instance of a pretrained BERT model.\n",
        "- the desidered number of outputs (logits) corresponding to the number of classes\n",
        "- the ```forward()``` takes mini-batch of token-ids, encodes them using BERT and outputs logits"
      ],
      "metadata": {
        "id": "LQym1Cl3rnhl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN86j0y9PIps"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLFm2rAtPIps"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(hf_dataset, sentence_field, label_field, tokenizer, **kwargs):\n",
        "    sentences = hf_dataset[sentence_field]\n",
        "    token_ids = tokenizer(sentences, return_tensors='pt', **kwargs)['input_ids']\n",
        "    labels = torch.as_tensor(hf_dataset[label_field])\n",
        "    dataset = TensorDataset(token_ids, labels)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvCj__6mPIps"
      },
      "outputs": [],
      "source": [
        "auto_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "tokenizer_kwargs = dict(truncation=True, padding=True, max_length=30, add_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRuvrZRfPIps"
      },
      "outputs": [],
      "source": [
        "train_dataset_float = train_dataset.map(lambda row: {'labels': [float(row['labels'])]})\n",
        "test_dataset_float = test_dataset.map(lambda row: {'labels': [float(row['labels'])]})\n",
        "\n",
        "train_tensor_dataset = tokenize_dataset(train_dataset_float, 'sentence', 'labels', auto_tokenizer, **tokenizer_kwargs)\n",
        "test_tensor_dataset = tokenize_dataset(test_dataset_float, 'sentence', 'labels', auto_tokenizer, **tokenizer_kwargs)\n",
        "\n",
        "generator = torch.Generator()\n",
        "train_loader = DataLoader(train_tensor_dataset, batch_size=4, shuffle=True, generator=generator)\n",
        "test_loader = DataLoader(test_tensor_dataset, batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_zRWx9gPIps"
      },
      "source": [
        "### Model Configuration & Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z1oEHNkPIps"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(41)\n",
        "bert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = BERTClassifier(bert_model, 128, n_outputs=1)\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZL4UQsVPIpt"
      },
      "outputs": [],
      "source": [
        "sbs_bert = StepByStep(model, loss_fn, optimizer)\n",
        "sbs_bert.set_loaders(train_loader, test_loader)\n",
        "sbs_bert.train(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uma227MPIpt"
      },
      "outputs": [],
      "source": [
        "sbs_bert.count_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOE-3A4jPIpt"
      },
      "outputs": [],
      "source": [
        "StepByStep.loader_apply(test_loader, sbs_bert.correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-YD9bvDPIpt"
      },
      "source": [
        "# Fine-Tuning with HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we said before, there is a BERT model for every task, and we need just to fine-tune it.\n",
        "\n",
        "HF makes at our disposal a **trainer** to do most of the fine-tuning work.\n",
        "\n",
        "- Pre-training tasks:\n",
        "  - Masked language mode (```BertForMaskedLM```)\n",
        "  - Next sentence prediction (```BertForNextSentencePrediction```)\n",
        "- Typical tasks:\n",
        "  - Sequence classification (```BertForSequenceClassification```)\n",
        "  - Token classification (```BertForTokenClassification```)\n",
        "  - Question answering (```BertForQuestionAnswering```)\n",
        "- Others:\n",
        " - Multiple choice (```BertForMultipleChoice```)\n",
        "\n",
        " In our case, we want to use ```DistilBERT``` for sequence classification.\n"
      ],
      "metadata": {
        "id": "gGOIU66vulFZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXihCcZSPIpt"
      },
      "source": [
        "## Sequence Classification (or Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsEGVD3wPIpt"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "torch.manual_seed(42)\n",
        "bert_cls = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7MQ1ApTPIpu"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "auto_cls = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "print(auto_cls.__class__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we need to do is to add a single linear layer (classifier) on top of the pooled output from the underlying model to produce the logits.\n",
        "\n",
        "We have the model, we prepare the dataset...\n",
        "\n",
        "We need to tokenize our HF's datasets, and we do it one row at the time creating a new column to contain the tokenized version of the sentence."
      ],
      "metadata": {
        "id": "2p0uI-ufwL5-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3eWB1c2PIpu"
      },
      "source": [
        "## Tokenized Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmJOKQcmPIpu"
      },
      "outputs": [],
      "source": [
        "auto_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "def tokenize(row):\n",
        "    return auto_tokenizer(row['sentence'],\n",
        "                          truncation=True,\n",
        "                          padding='max_length',\n",
        "                          max_length=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzW2xAeDPIpu"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCToy9BpPIpv"
      },
      "outputs": [],
      "source": [
        "print(tokenized_train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJGHlBQ9PIpv"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "we select only the need columns and return them as tensors\n",
        "'''\n",
        "\n",
        "tokenized_train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsQJ3GjfPIpv"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWBcmaN1PIpv"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHy5s29tPIpv"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(model=bert_cls, train_dataset=tokenized_train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7pDhbwdPIpw"
      },
      "outputs": [],
      "source": [
        "trainer.args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMVuTOSUPIpw"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./output', # Where to save the output files\n",
        "    run_name=\"bert_experiment\",  # Set a unique name for this run\n",
        "    logging_dir=\"./logs\",  # Directory for logging\n",
        "    report_to=[\"none\"],\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=300,\n",
        "    logging_steps=300,\n",
        "    gradient_accumulation_steps=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the batch size, it is only one, but we keep accumulating the gradients for 8 steps, it is a way to simulate 8 size batches."
      ],
      "metadata": {
        "id": "06_hGLmWxmQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFxIhoybPIpw"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can specify a class to compute the desired metrics and pass it to the Trainer instance"
      ],
      "metadata": {
        "id": "YSMLyiZNyKsK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKo42k7RPIpw"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=bert_cls,\n",
        "                  args=training_args,\n",
        "                  train_dataset=tokenized_train_dataset,\n",
        "                  eval_dataset=tokenized_test_dataset,\n",
        "                  compute_metrics=compute_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wdDiBP4PIpw"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_i6kvF5PIpx"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can save it, and use later."
      ],
      "metadata": {
        "id": "a76GnBMVze2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KOMV6iQPIpx"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('bert_alice_vs_wizard')\n",
        "os.listdir('bert_alice_vs_wizard')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WpIt7WvPIpx"
      },
      "outputs": [],
      "source": [
        "loaded_model = AutoModelForSequenceClassification.from_pretrained('bert_alice_vs_wizard')\n",
        "loaded_model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrmSH5kSPIpx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "loaded_model.to(device)\n",
        "loaded_model.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_6MnEPTPIpx"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you remember last time, we started with a peculiar sentence.\n",
        "Now we are able to classify it (also before, you can try).\n",
        "\n",
        "We tokenize it, we send to the right device and then we evaluate"
      ],
      "metadata": {
        "id": "_QuwUJzszsrv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_dt7C28PIpy"
      },
      "outputs": [],
      "source": [
        "sentence = 'Down the yellow brick rabbit hole'\n",
        "tokens = auto_tokenizer(sentence, return_tensors='pt')\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAJtVv5fPIpy"
      },
      "outputs": [],
      "source": [
        "print(type(tokens))\n",
        "tokens.to(loaded_model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs4-3g2DPIpy"
      },
      "outputs": [],
      "source": [
        "loaded_model.eval()\n",
        "logits = loaded_model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_81JJd68PIpy"
      },
      "outputs": [],
      "source": [
        "logits.logits.argmax(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOIE2XY-PIpy"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make it more efficient using pipelines\n",
        "\n",
        "There are many pipelines, one for each task:\n",
        "- ```TextClassificationPipeline```\n",
        "- ```TextGenerationPipeline```\n",
        "\n",
        "Every pipeline takes at least two argument:\n",
        "- a model\n",
        "- a tokenizer\n",
        "\n",
        "Now, we can make predictions using the **original sentences**"
      ],
      "metadata": {
        "id": "LF4mUAdf0Oqw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTdPgNWAPIpy"
      },
      "outputs": [],
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "device_index = loaded_model.device.index if loaded_model.device.type != 'cpu' else -1\n",
        "classifier = TextClassificationPipeline(model=loaded_model,\n",
        "                                        tokenizer=auto_tokenizer,\n",
        "                                        device=device_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkhu6QaOPIpz"
      },
      "outputs": [],
      "source": [
        "classifier(['Down the Yellow Brick Rabbit Hole', 'Alice rules!'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57PmBUrZPIpz"
      },
      "outputs": [],
      "source": [
        "loaded_model.config.id2label = {0: 'Wizard', 1: 'Alice'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xsmzplHPIpz"
      },
      "outputs": [],
      "source": [
        "classifier(['Down the Yellow Brick Rabbit Hole', 'Alice rules!'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPv9QGnPPIpz"
      },
      "source": [
        "## More Pipelines\n",
        "\n",
        "It is possible to use pre-trained pipeline for **typical tasks** like sentiment analysis, without any fine-tuning.\n",
        "\n",
        "*check the pipeline documentation on HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orvV3n9wPIpz"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "sentiment = pipeline('sentiment-analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGiwPvGfPIp0"
      },
      "outputs": [],
      "source": [
        "sentence = train_dataset[0]['sentence']\n",
        "print(sentence)\n",
        "print(sentiment(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x456QWjEPIp0"
      },
      "outputs": [],
      "source": [
        "from transformers.pipelines import SUPPORTED_TASKS\n",
        "# UPDATED\n",
        "###########################################################\n",
        "# sentiment-analysis was replaced by text-classification\n",
        "# in the dictionary of supported tasks\n",
        "# SUPPORTED_TASKS['sentiment-analysis']\n",
        "SUPPORTED_TASKS['text-classification']\n",
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SUPPORTED_TASKS"
      ],
      "metadata": {
        "id": "w_I2GVwM7wDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk3TWVXxPIp0"
      },
      "outputs": [],
      "source": [
        "SUPPORTED_TASKS['text-generation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpfW8LWvPIp0"
      },
      "source": [
        "# GPT-2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **G**enerative **P**retrained **T**ransfomer 2 is able to generate text.\n",
        "\n",
        "It was trained to fill the in the blanks at the end of the sentences, effectively predicting the next word in a given sentence.\n",
        "\n",
        "This taks is exactly what a transformer Decoder does, and this what GPT-2 is, a transfomer decoder.\n",
        "\n",
        "40GB of internet text, 8 millions of web pages, 48 layers, 12 attention heads, and 1600 hidden dimensions, 1.5 billion parameters (Nov. 2019)."
      ],
      "metadata": {
        "id": "iOrg59YR1mTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Gs2m5QtPIp0"
      },
      "outputs": [],
      "source": [
        "text_generator = pipeline(\"text-generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNIhhyloPIp1"
      },
      "outputs": [],
      "source": [
        "text_generator.model.config.task_specific_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAlA7qvJPIp1"
      },
      "outputs": [],
      "source": [
        "base_text = \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the bank,\n",
        "and of having nothing to do:  once or twice she had peeped into the book her\n",
        " sister was reading, but it had no pictures or conversations in it, `and what\n",
        " is the use of a book,'thought Alice `without pictures or conversation?'\n",
        " So she was considering in her own mind (as well as she could, for the hot day\n",
        " made her feel very sleepy and stupid), whether the pleasure of making a\n",
        " daisy-chain would be worth the trouble of getting up and picking the daisies,\n",
        " when suddenly a White Rabbit with pink eyes ran close by her.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGsxW7xVPIp1"
      },
      "outputs": [],
      "source": [
        "result = text_generator(base_text, max_length=250)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb1uve8LPIp1"
      },
      "source": [
        "## Hold-on, we can fine-tune GPT2 too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBjmamx_PIp1"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKf7qtJkPIp1"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(path='csv', data_files=['texts/alice28-1476.sent.csv'], quotechar='\\\\', split=Split.TRAIN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRxp47A-PIp2"
      },
      "outputs": [],
      "source": [
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "split_dataset = shuffled_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset, test_dataset = split_dataset['train'], split_dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVF5IzXsPIp2"
      },
      "outputs": [],
      "source": [
        "auto_tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "def tokenize(row):\n",
        "    return auto_tokenizer(row['sentence'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GPT2 uses a different pre-trained tokenizer based on Byte-Pair encoding\n",
        "- we don't need padding, we need to generate text, and we don't want to write something after many padding tokens.\n",
        "- we remove (below) the source and sentence columns (as before)\n",
        "- then we pack sentence together, concateneting the inputs and chunk them into blocks."
      ],
      "metadata": {
        "id": "5ayhMfov4Qgc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUBOlXsqPIp2"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(tokenize, remove_columns=['source', 'sentence'], batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize, remove_columns=['source', 'sentence'], batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-4cJ4pAPIp2"
      },
      "outputs": [],
      "source": [
        "list(map(len, tokenized_train_dataset[0:6]['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKWqqWJ3PIp2"
      },
      "source": [
        "### \"Packed\" Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7aJwvXsPIp3"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/block_tokens.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lAMC3OtPIp3"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py\n",
        "def group_texts(examples, block_size=128):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TirMA67-PIp3"
      },
      "outputs": [],
      "source": [
        "lm_train_dataset = tokenized_train_dataset.map(group_texts, batched=True)\n",
        "lm_test_dataset = tokenized_test_dataset.map(group_texts, batched=True)\n",
        "lm_train_dataset.set_format(type='torch')\n",
        "lm_test_dataset.set_format(type='torch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwOa4nluPIp3"
      },
      "outputs": [],
      "source": [
        "print(lm_train_dataset[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkJM5tc3PIp3"
      },
      "outputs": [],
      "source": [
        "\n",
        "len(lm_train_dataset), len(lm_test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgLxoOb-PIp3"
      },
      "source": [
        "## Model Configuration & Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT2 is a causal language modeling, therefore we use it to import"
      ],
      "metadata": {
        "id": "JExINZlA5Rmh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9NLtw1APIp4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "print(model.__class__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fI5PE_nPIp4"
      },
      "outputs": [],
      "source": [
        "model.resize_token_embeddings(len(auto_tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta_7BbrVPIp4"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./output', # Where to save the output files\n",
        "    run_name=\"gpt2_experiment\",  # Set a unique name for this run\n",
        "    logging_dir=\"./logs\",  # Directory for logging\n",
        "    report_to=[\"none\"],\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=50,\n",
        "    logging_steps=50,\n",
        "    gradient_accumulation_steps=4,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model,\n",
        "                  args=training_args,\n",
        "                  train_dataset=lm_train_dataset,\n",
        "                  eval_dataset=lm_test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WQN-YhePIp4"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ru9UY-NPIp5"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYsl0aOiPIp5"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhkK5TIFPIp5"
      },
      "outputs": [],
      "source": [
        "device_index = model.device.index if model.device.type != 'cpu' else -1\n",
        "gpt2_gen = pipeline('text-generation', model=model, tokenizer=auto_tokenizer, device=device_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF0RfSp4PIp5"
      },
      "outputs": [],
      "source": [
        "result = gpt2_gen(base_text, max_length=250)\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQG_yBYhPIp6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}